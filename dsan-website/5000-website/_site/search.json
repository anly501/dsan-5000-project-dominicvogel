[
  {
    "objectID": "naive_bayes/naive_bayes.html",
    "href": "naive_bayes/naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes\n\nOverview of Naive Bayes\nNaive Bayes is a machine learning algorithm based off of Bayes’ theorem. Bayes’ Theorem is a statistical concept that computes the probability of an event based on the occurence of a prior event. Bayes’ Theorem is calculated below:\n\n\n\n\n\nThe theorem states that the probability of an event A occurring given an event B occurred is equal to the probability event B occurs given event A occurred, multiplied by the probability event A occurs, and divided by the probability event B occurs. This allows us to conditionally update the probability of our goal event.\nNaive Bayes assumes conditional independence between all features. Because of this assumption, the algorithm gets significantly simplified, reducing the necessary probability calculations. Despite it being less computationally complex than other tools, Naive Bayes is still an incredibly powerful algorithm and can be used to gain impressive results.\nFor my project’s sake, I hope to use Naive Bayes to get a better understanding of the fundamental relationships between the different features in my data. Hopefully, I can discover an optimal set of features that serve as a great classifier for both my tournament and Creighton text data.\n\n\nVariants of Naive Bayes\n\nGaussian Naive Bayes- Gaussian Naive Bayes is used on datasets that contain continuous, normally distributed features. I will be using this variant on my historical tournament data to try to classify which teams made the tournament. While my dataset contains different types of features, I will be selecting the ones that make the most sense for the Gaussian method below.\nMultinomial Naive Bayes- Multinomial Naive Bayes is used on discrete datasets. Most commonly, it is used on text data classification. After vectorizing my Creighton data, I will run the Multinomial Naive Bayes method to try to determine which word(s) are most beneficial in determining whether or not Creighton scored from a particular text description.\nBernoulli Naive Bayes- Bernoulli Naive Bayes is often used on a dataset with binary features. It is used as a classification method on data that is assumed to come from the Bernoulli distribution. While it is not used in my project, there could definitely be use cases in future exploration on the topic of college basketball.\n\n\nImports and Dataset\n\n\nCode\nimport pandas as pd\nimport warnings\nimport random\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport itertools\nfrom scipy.stats import spearmanr\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\n\nUtilizing the Naive Bayes method requires quite a few imports. Of course, pandas is used to import and handle our data. Our usual plotting libraries are used for visualization. The scipy library is used for Feature Selection, as I aim to find the optimal subset of features for the Naive Bayes method. The sklearn library contains the necessary functions to actually run the Naive Bayes method as well as analyze it.\n\n\n\nPreparing Data for Naive Bayes\n\nNCAA Record Data\n\n\nCode\nbart = pd.read_csv(\"../data/cbb.csv\")\nto_drop = [\"TotalRank\", \"TotalOffenseRank\", \"TotalDefenseRank\", \"ADJOE\", \"ADJDE\", \"TEAM\", \"POSTSEASON\", \"SEED\", \"YEAR\", \"G\"]\nbart.drop(columns= to_drop, inplace= True)\nbart['CONF'] = pd.Categorical(bart['CONF'])\nbart['CONF'] = bart['CONF'].cat.codes\n\n#Too many to run\nmore_drop = [\"ORB\", \"DRB\", \"2P_O\", \"2P_D\", \"3P_O\", \"3P_D\"]\nbart.drop(columns= more_drop, inplace= True)\nbart.columns\n\n\nIndex(['CONF', 'W', 'BARTHAG', 'EFG_O', 'EFG_D', 'TOR', 'TORD', 'FTR', 'FTRD',\n       'ADJ_T', 'WAB', 'YearRank', 'YearOffenseRank', 'YearDefenseRank',\n       'Made_Tournament'],\n      dtype='object')\n\n\nTo start, I imported our historic NCAA team data. While there are lots of relevant features that would make for interesting results, I ended up having to trim down which features would be tested for merit score combinations as to save time and computing power. I filtered out some of the columns that, from our correlation investigation, were less likely to have significant impacts on classification.\n\n\nCode\nx= bart.drop(\"Made_Tournament\", axis =1)\ny= bart[\"Made_Tournament\"]\n\nX_train, X_split, y_train, y_split = train_test_split(x, y, test_size=0.3, random_state=42)\n\nX_test, X_val, y_test, y_val = train_test_split(X_split, y_split, test_size=0.33, random_state=42) \n\nX_train.head()\n\n\n\n\n\n\n\n\n\nCONF\nW\nBARTHAG\nEFG_O\nEFG_D\nTOR\nTORD\nFTR\nFTRD\nADJ_T\nWAB\nYearRank\nYearOffenseRank\nYearDefenseRank\n\n\n\n\n944\n21\n14\n0.1859\n45.9\n46.6\n20.4\n18.6\n36.1\n34.9\n66.4\n-13.7\n299\n332\n195\n\n\n199\n4\n22\n0.7703\n52.2\n47.3\n15.4\n18.4\n34.4\n36.0\n67.3\n-1.2\n70\n21\n191\n\n\n2967\n3\n21\n0.5394\n48.6\n47.2\n18.6\n16.4\n30.5\n30.8\n61.4\n-5.6\n152\n214\n119\n\n\n1270\n29\n7\n0.1371\n45.1\n52.9\n19.7\n21.4\n43.2\n43.0\n72.7\n-16.5\n325\n311\n310\n\n\n801\n18\n10\n0.1082\n45.8\n51.3\n19.5\n18.2\n37.0\n48.5\n68.7\n-16.8\n338\n314\n335\n\n\n\n\n\n\n\nNext, I split the data into Training, Testing, and Validation sets. I ended up putting 70% of the data into the training set, 20% into testing, and the final 10% into validation. This prevents overfitting in our model as we are only training our classifier on 70% of our total data. The remaining data serves as a way to validate that our model is actually picking up on meaningful relationships and will be useful in the future, rather than just overfitting the specific set it was trained on.\n\n\nCreighton Text Data\n\n\nCode\ncreighton = pd.read_csv(\"../data/creighton_cleaned.csv\")\n\n#Only want description and Creighton_Score\ncreighton = creighton[[\"Creighton_Score\", \"description\"]]\n\nC_train, C_split = train_test_split(creighton,test_size=0.3, random_state=42)\n\nC_test, C_val = train_test_split(C_split, test_size=0.33, random_state=42)\n\nC_train.head()\n\n\n\n\n\n\n\n\n\nCreighton_Score\ndescription\n\n\n\n\n540\nFalse\nRyan Kalkbrenner Offensive Rebound.\n\n\n7224\nFalse\nFoul on Shereef Mitchell.\n\n\n4950\nFalse\nRyan Nembhard missed Free Throw.\n\n\n9717\nFalse\nRocket Watts made Free Throw.\n\n\n1146\nFalse\nFoul on Shereef Mitchell.\n\n\n\n\n\n\n\nNext, I repeated the same process for the Creighton text data. While this dataset contains quite a few features, there were only two needed for the Naive Bayes method. Those are 1) the textual description of each play and 2) whether or not that play resulted in Creighton scoring. Once again, the data was split into 70% training, 20% testing, 10% validation.\n\n\n\nFeature Selection for NCAA Record Data\n\n\nCode\ndef merit(x,y,correlation=\"pearson\"):\n    # x=matrix of features \n    # y=matrix (or vector) of targets \n    # correlation=\"pearson\" or \"spearman\"\n    \n  k = x.shape[1]\n  if correlation == \"spearman\":\n    rho_xx = np.mean(spearmanr(x, x, axis= 0)[0])\n    rho_xy = np.mean(spearmanr(x, y, axis = 0) [0])\n  elif correlation == \"pearson\":\n    rho_xx = np.mean(np.corrcoef(x, x, rowvar = False))\n    rho_xy = np.mean(np.corrcoef(x, y, rowvar = False))\n  else:\n    print(\"Please set correlation to either 'pearson' or 'spearman' and try again.\")\n  merit_numerator = k* np.absolute(rho_xy)\n  merit_denominator = np.sqrt(k + k * (k-1) * np.absolute(rho_xx)) \n  merit_score = merit_numerator / merit_denominator\n  return merit_score\n\ndef maximize_CFS(x,y):\n  max_merit = 0\n  optimal_subset = None\n  num_features = x.shape[1]\n  list1 = [*range(0, num_features)] \n  for L in range(1, len(list1) + 1): \n    subsets = itertools.combinations(list1, L)\n    for subset in subsets:\n      subsetx = x.iloc[:,list(subset)]\n      subset_merit = merit(subsetx, y)\n      if subset_merit &gt; max_merit:\n        max_merit = subset_merit\n        optimal_subset = list(subset)\n        \n  return optimal_subset\n\n\nHere, I created two separate functions.\nThe first, merit, calculates a merit score that essentially calculates the relationship between the chosen features and the target feature (y). It defaults to Pearson correlation but can use Spearman correlation for the calculations if specified.\nThe second, maximize_CFS, takes two matrices (x and y), and loops through all possible combinations of features. For each combination, it records its merit score and at the end returns the subset of features with the highest merit score. The goal of these function combinations is to find the optimal subset of features that has the strongest correlation strength with the target feature.\n\n\nCode\noptimal_subset = maximize_CFS(X_train, y_train)\nbart.iloc[:, optimal_subset]\n\n\n\n\n\n\n\n\n\nW\nBARTHAG\nWAB\n\n\n\n\n0\n33\n0.9531\n8.6\n\n\n1\n36\n0.9758\n11.3\n\n\n2\n33\n0.9375\n6.9\n\n\n3\n31\n0.9696\n7.0\n\n\n4\n37\n0.9728\n7.7\n\n\n...\n...\n...\n...\n\n\n3518\n27\n0.7369\n-1.2\n\n\n3519\n27\n0.8246\n-2.0\n\n\n3520\n28\n0.8065\n-0.3\n\n\n3521\n29\n0.8453\n-0.5\n\n\n3522\n31\n0.8622\n1.1\n\n\n\n\n3523 rows × 3 columns\n\n\n\nFirst, we run the maximize_CFS function on our features for the NCAA record data. After running it, we see that the optimal subset to maximize our correlation with making the tournament are the features W, BARTHAG, and WAB. These should come as no surprise after inspecting the correlation matrices from the EDA section of this project. BARTHAG is specifically computed as a predictive metric while WAB seeks to quantify how worthy of the tournament a specific team is. The third column, Wins, also tracks as teams that win more games should have a higher probability of making the tournament. Obviously, the relationship here isn’t perfect as teams in major conferences will play much harder schedules than teams in small conferences (and will thus be expected to win less games). However, wins do serve as a good proxy of on-court results.\n\n\nCode\nto_keep = [\"W\", \"BARTHAG\", \"WAB\"]\nX_optimal_train = X_train[to_keep]\nX_optimal_val = X_val[to_keep]\nX_optimal_test = X_test[to_keep]\n\nmerit(X_optimal_train, y_train)\n\n\n0.826238481861058\n\n\nHere, we calculate the merit score of this subset to be 0.83, which is a pretty strong correlation between the feature set and our target feature. These three features contain a lot of relevant information needed to predict whether or not a team ends up making the tournament.\n\n\nFeature Selection for Creighton Text Data\nFor the Creighton text data, I sought to use a similar method of feature selection. Obviously, features look a bit different when working with a text dataset, but, by using the Vectorizer package to columnize the text by word frequency, this feature selection method can be applied.\n\n\nCode\nCX_train = C_train[\"description\"].tolist()\ncy_train = C_train[\"Creighton_Score\"]\n\n\n\n\nCode\nlabel_mapping = {True: 1, False: 0}\n\ncy_train = cy_train.map(label_mapping)\ncy_train.head()\n\n\n540     0\n7224    0\n4950    0\n9717    0\n1146    0\nName: Creighton_Score, dtype: int64\n\n\nHere, I mapped the “Creighton_Score” column to 1 for plays where they scored and 0 for plays they did not. The column was initially set up as a boolean column before. This column was set as our target for Multinomial Naive Bayes.\n\n\nCode\nvectorizer = CountVectorizer(stop_words=list(ENGLISH_STOP_WORDS))\nCX_train_vectorized = vectorizer.fit_transform(CX_train)\n\ntext_df = pd.DataFrame(CX_train_vectorized.toarray())\ntext_df.describe()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n\n\n\n\ncount\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n...\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n7789.000000\n\n\nmean\n0.002953\n0.002824\n0.002696\n0.006933\n0.007446\n0.004494\n0.001155\n0.001027\n0.004622\n0.004622\n...\n0.000128\n0.001669\n0.003852\n0.002054\n0.000257\n0.001027\n0.006548\n0.003852\n0.000514\n0.001926\n\n\nstd\n0.054264\n0.053074\n0.051857\n0.082980\n0.085976\n0.066887\n0.033975\n0.032034\n0.067832\n0.096040\n...\n0.011331\n0.040822\n0.061945\n0.045279\n0.016023\n0.032034\n0.080658\n0.061945\n0.022657\n0.043844\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n2.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 520 columns\n\n\n\nNext, I split the training text data into different columns based on each word. After removing stopwords, it seems as though 519 different words appeared at least once throughout our test data.\n\n\nCode\ncolumn_sums = text_df.sum()\ncolumns_to_remove = column_sums[column_sums &lt;= 700].index\ntext_df = text_df.drop(columns=columns_to_remove)\ntext_df.shape\n\n\n(7789, 12)\n\n\nAt this step, I had to greatly reduce the number of features we would be working with. In an ideal world, with a much more powerful machine, it would be great to test every combination of our 519 words to determine an optimal selection. There are likely some more specific words (i.e. player names) that might lead to a better classifier performance. However, I had to reduce our feature size to the 12 most frequently occurring words. Each appeared at least 700 times, meaning that there is a lot of data to work with. Because these descriptions are scraped from ESPN, each play description is generated from a preset group of templates. This is why, despite 12 words occurring 700+ times each, there were only 519 total words in the training corpus.\n\n\nCode\n#Edit function to work with numpy arrays\ndef maximize_CFS(x, y):\n    max_merit = 0\n    optimal_subset = None\n    num_features = x.shape[1]\n    list1 = [*range(0, num_features)]\n    for L in range(1, len(list1) + 1):\n        subsets = itertools.combinations(list1, L)\n        for subset in subsets:\n            subsetx = x[:, list(subset)]\n            subset_merit = merit(subsetx, y)\n            if subset_merit &gt; max_merit:\n                max_merit = subset_merit\n                optimal_subset = list(subset)\n\n    return optimal_subset\n\n\nI had to tweak the function slightly to work with numpy arrays, which is the form our training data took on.\n\n\nCode\ncx = text_df.values\ncy = cy_train.values\noptimal_subset_indices = maximize_CFS(cx, cy)\nprint(optimal_subset_indices)\nx_opt = text_df.iloc[:, optimal_subset_indices]\nprint(x_opt)\n\n\n[3, 11]\n      178  457\n0       0    0\n1       0    0\n2       1    1\n3       1    1\n4       0    0\n...   ...  ...\n7784    0    0\n7785    1    1\n7786    0    0\n7787    0    0\n7788    0    0\n\n[7789 rows x 2 columns]\n\n\nAfter running the all feature combinations of our 13 features, the optimal subset turns out to be just indexes 3 and 11, which ends up being columns 178 and 458 from our original 519 column set.\n\n\nCode\nword_index = 178\nword_index2 = 457\nvocabulary = vectorizer.vocabulary_\nword = next(word for word, index in vocabulary.items() if index == word_index)\nword2 = next(word for word, index in vocabulary.items() if index == word_index2)\nprint(\"Word at column 178:\", word)\nprint(\"Word at column 457:\", word2)\n\n\nWord at column 178: free\nWord at column 457: throw\n\n\nTo determine what this word actually was, I indexed the corresponding words, which turned out to be “free” and “throw”. This implies that having these words in the play description is the feature combination that best correlates to whether or not Creighton scores on that particular play.\n\n\nNaive Bayes with NCAA Record Data\nNow that we have our optimal subset of features (that being Wins, BARTHAG, and Wins Above Bubble), we can begin to actually implement Naive Bayes classification. As mentioned at the start of this page, we will be using Gaussian Naive Bayes on our NCAA Data.\n\n\nCode\ngnb_model = GaussianNB()\ngnb_model.fit(X_optimal_train, y_train)\n\ny_pred_train = gnb_model.predict(X_optimal_train)\ny_pred_val = gnb_model.predict(X_optimal_val)\ny_pred_test = gnb_model.predict(X_optimal_test)\n\naccuracy = accuracy_score(y_val, y_pred_val)\nprint(f\"Validation Accuracy: {accuracy}\")\n\n\nValidation Accuracy: 0.8882521489971347\n\n\nFrom running the model, we can determine it has 88% accuracy when used on the validation set.\n\n\nCode\nconf_matrix = confusion_matrix(y_val, y_pred_val)\n\nsns.reset_orig()\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', linewidths=0.5, cbar=False, xticklabels=['Missed Tournament', 'Made Tournament'], yticklabels=['Missed Tournament', 'Made Tournament'])\n\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\nAfter generating the confusion matrix from our validation set, we can see just how well our model is doing at classification. The model does a fantastic job at predicting teams that miss the tournament, but does a much less ideal job when it comes to actually predicting tournament teams.\n\n\nCode\nprecision = precision_score(y_val, y_pred_val)\nrecall = recall_score(y_val, y_pred_val)\nf1 = f1_score(y_val, y_pred_val)\n\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n\n\nPrecision: 0.66\nRecall: 0.85\nF1 Score: 0.75\n\n\nFor this model, our Precision was 66%. This means that ~2/3 of the teams that actually made the tournament were predicted by our model to (57/86). The model’s recall was 85%. This means that 57 out of the 67 teams it predicted to make the tournament actually did end up making it. The harmonic mean of those two values (F1 Score) was 0.75.\nOverall, I am moderately impressed with this model. While it leaves something to be desired in terms of precision, it does a really good job at filtering out teams that won’t end up making the tournament. Of the teams it guessed would miss who actually made it in, my hypothesis is that most of them were automatic qualifiers from winning their conference tournament. In further analysis, I would like to filter the auto-qualifiers from the dataset, as they kind of break the system. Many smaller conferences end up sending teams with middling records and metrics as their sole representative each year. While this is part of what makes the tournament so fun, I think it also makes it hard to create a model, as those teams would not have been selected had they not won their tournament.\n\n\nNaive Bayes with Creighton Text Data\nFor the Creighton data, we simply are using the words “free” and “throw” to classify whether or not Creighton scored on a particular play. I have my doubts about how well this model will work, but it may prove me wrong.\n\n\nCode\nCX_test = C_test[\"description\"]\nCy_test = C_test[\"Creighton_Score\"]\nCX_test_timeout = CX_test.str.lower().str.count(\"throw\")\nCX_test_free = CX_test.str.lower().str.count(\"free\")\nCX_test_timeout.value_counts()\nCX_test_free.value_counts()\n\n\n0    2021\n1     216\nName: description, dtype: int64\n\n\nFirst, I wanted to print out the counts of assisted from our test set. The word “throw” appears 206 times out of 2237 possible occurrences (~9,2%). The word “free” appears 216 times out of 2237 occurrences (~ 9.7%) These words also almost always appear together, as in the term ‘free throw’.\n\n\nCode\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    CX_test_vectorized = vectorizer.transform(CX_test).toarray()\n    text_df_test = pd.DataFrame(CX_test_vectorized)\n    creighton_test_x = text_df_test.iloc[:, optimal_subset_indices]\n    nb_model = MultinomialNB()\n    nb_model.fit(x_opt, cy_train)\n    Cy_pred = nb_model.predict(creighton_test_x)\n    accuracy = accuracy_score(Cy_test.values, Cy_pred)\n    print(\"Accuracy of the Naive Bayes model:\", accuracy)\n\n\nAccuracy of the Naive Bayes model: 0.8828788556101922\n\n\nAs mentioned before, the type of Naive Bayes used for text data is Multinomial Naive Bayes. After creating the model from our training data, it then is applied to our test set. Initially, I was surprised to see 88% accuracy, but for reasons I will get into below it turned out not to actually be very surprising.\n\n\nCode\nlabel_mapping = {True: 1, False: 0}\nCy_test = C_test[\"Creighton_Score\"].map(label_mapping)\nCy_test.value_counts()\n\n\n0    1975\n1     262\nName: Creighton_Score, dtype: int64\n\n\nFrom our test data, I wanted to see how often Creighton actually scored. They scored on 262 of the 2237 plays in the test data (~12.3%). Since this frequency is rather low, I assume our model picked up on this and classified most of our test data as not being a Creighton scoring play.\n\n\nCode\nconf_matrix = confusion_matrix(Cy_test, Cy_pred)\n\nplt.figure()\nsns.heatmap(conf_matrix.T, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['No Score', 'Creighton Score'], yticklabels=['No Score', 'Creighton Score'])\nplt.xlabel('True Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Confusion Matrix Heatmap')\nplt.show()\n\n\n\nIn fact, this ended up being entirely the case. The model went ahead and predicted that there was no Creighton score on every single play. Because of this, the accuracy is high on first glance, but the precision and recall are non-existent. This model does a terrible job at prediction and is significantly underfitted. This is interesting because a “free throw” is at least a high potential scoring play. Most teams shoot well above 50% from the line, meaning that each time those words appear, it is more likely than not that someone scored. However, devoid of all context, I guess you don’t know which team is shooting the free throw and thus the classifier sees better value in guessing it’s either the other team shooting or a Creighton miss.\n\n\nConclusion\nI was pleasantly satisfied with the Naive Bayes results of my NCAA record data, while the classification of the text data left a lot to be desired. Funnily enough, both had similar accuracy scores, which is a fantastic reminder that accuracy score alone can be incredibly deceiving. The text model was about as worthless as it could possibly be and definitely could use some fine-tuning. One thing that I think would have really benefited the text model is if I was able to choose an optimal feature set from a larger pool of features. Having to narrow the features down to 12 (especially when much of the data had hundreds of occurrences) , made it harder to find an optimal set. Hopefully, one day I will have a more powerful computer (or more patience) that will allow me to do so."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000 Project Homepage",
    "section": "",
    "text": "This is Dominic’s homepage. Please navigate to the other tabs.\nName: Dominic Vogel\nNetID: dav44"
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html",
    "href": "dimensionality_reduction/dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Dimensionality Reduction\n\nIntroduction to Dimensionality Reduction\nOn this tab, I will introduce the concept of dimensionality reduction and apply it to my NCAA Team dataset. Dimensionality reduction is a machine learning technique that seeks to reduced the number of features within a dataset while retaining as much of the important information as possible. I will use two different techniques: Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE). PCA is a linear dimensionality reduction technique which seeks to retain the linear combinations of features that explain the most variance within the data. t-SNE is non-linear and focuses on preserving pairwise similarities between data points.\n\nImports and Dataset\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport plotly.express as px\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.manifold import TSNE\n\nbart = pd.read_csv(\"../data/cbb.csv\")\n\n\nPer usual, we will use pandas to import our BartTorvik dataset. matplotlib and plotly are used for visualizations of our dimensionality reductions, with the sklearn package allows us to actually run these techniques.\n\n\nCode\nbart['CONF'] = pd.Categorical(bart['CONF'])\nbart['CONF'] = bart['CONF'].cat.codes\nto_drop = [\"POSTSEASON\", \"SEED\", \"TEAM\", \"ADJOE\", \"ADJDE\", \"TotalOffenseRank\", \"TotalDefenseRank\", \"TotalRank\", \"YearRank\", \"YEAR\"]\nbart= bart.drop(columns= to_drop)\nbart.to_csv(\"../data/bart_dimensionality.csv\", index=False)\n\n\nBefore performing any dimensionality reduction, I first dropped columns that I deemed to not perfectly explain our data. “POSTSEASON” and “SEED” have perfect overlap with the Made Tournament column, as teams that didn’t make the tournament all share the same value in each column. The Conference column was label encoded, so it can be expressed in a linear combination. Lastly, I dropped ADJOE, ADJDE, and the TotalRanks based off of these efficiences as they are not meant to be compared across years. Instead, I retained the YearRank columns for both ADJOE and ADJDE.\n\n\n\nPrincipal Component Analysis (PCA)\n\n\nCode\nscaler = StandardScaler()\n\nbart_standardized = scaler.fit_transform(bart)\n\n\nTo begin the PCA process, I first standardized each column as to have a mean of 0 and a standard deviation of 1.\n\n\nCode\ncovariance_matrix = np.cov(bart_standardized, rowvar= False)\n\n\nHere, our covarience matrix was constructed. Since the dataset still has a lot of features (even after dropping quite a few), the covariance matrix is quite large.\n\n\nCode\neigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\nprint(\"Eigenvalues\\n\",\"----------------------\")\nprint(eigenvalues)\n\n\nEigenvalues\n ----------------------\n[7.39415634e+00 3.14860160e+00 1.64446343e+00 1.10375112e+00\n 1.09419835e+00 9.45839441e-01 8.22571742e-01 7.75370854e-01\n 7.26315659e-01 6.54569650e-01 3.29252627e-01 4.41312869e-01\n 5.77000256e-01 5.51073364e-01 5.60252448e-01 1.16680795e-01\n 8.42141888e-02 2.31969669e-02 6.68654999e-03 2.50466948e-03\n 3.94960188e-03]\n\n\nEigenvalues and Eigenvectors were then generated, as to help with our dimensionality reduction. The eigenvectors represent directions within the feature space while the eigenvalues represent the magnitudes of those directions.\n\n\nCode\nindex_sort = np.argsort(eigenvalues)[::-1]\neigenvalue_sort = eigenvalues[index_sort]\n\n\n\n\nCode\ncumulative_explained_variance = np.cumsum(eigenvalue_sort) / sum(eigenvalue_sort)\ndesired_variance = 0.8\nnum_components = np.argmax(cumulative_explained_variance &gt;= desired_variance) + 1\nprint(f\"Number of components to capture {desired_variance * 100}% variance: {num_components}\")\npca = PCA(n_components=num_components)\nbart_pca = pca.fit_transform(bart_standardized)\ncomponents = pca.components_\n\n\nNumber of components to capture 80.0% variance: 8\n\n\nAfter sorting the eigenvalues, we are then able to determine which linear combinations of features explain the most about our dataset’s variance. I set the threshold for explained variance to be 80%. This number turned out to be 8, meaning that our top 8 principal components explain over 80% of our total variance. After this, these 8 components are used to fit the PCA model.\n\n\nCode\ncomponents_df = pd.DataFrame(components, columns=bart.columns)\ncomponents_df.head()\ncomponents_df.to_csv(\"../data/pca_components.csv\", index=False)\n\n\nHere, I created a dataset that takes each of the 8 components fit to the PCA model, and shows us the makeup of each linear combination. This will be looked deeper into later on this page.\n\n\nCode\nplt.figure()\nplt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', color='#612bd4')\nplt.title('Cumulative Explained Variance')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.show()\n\n\n\nHere, we are able to see how much of the variance in our dataset is explained by adding a specific number of principal components. As you can see, we cross the 80% threshold with 8 components, and it seems like using 12 components would have been right at about the 90% threshold.\n\n\nCode\nexplained_variance_ratio = pca.explained_variance_ratio_\nprint(\"Explained Variance Ratio for Each Component:\")\nprint(explained_variance_ratio)\n\n\nExplained Variance Ratio for Each Component:\n[0.35200274 0.14989085 0.07828555 0.05254466 0.05208989 0.04502719\n 0.03915896 0.03691194]\n\n\nBy printing the explained variance ratios, we can see how much each individual component added to our total explained variance. Our first principal component explained 35% of the total variance in our dataset, while the eighth explained 3.6%.\n\n\nCode\nplt.figure()\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, color='#FFB6C1')\nplt.title('Plot of Principal Components and Explained Variance')\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Variance Ratio')\nplt.show()\n\n\n\nThis further visualizes the difference in explained variance between each of our 8 components.\n\n\nCode\ncolors = ['blue', 'red']\ncmap = ListedColormap(colors)\n\nplt.figure()\nscatter = plt.scatter(bart_pca[:, 0], bart_pca[:, 1], c=bart['Made_Tournament'], cmap=cmap, alpha=0.6)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Scatter Plot of PCA Components')\ncbar = plt.colorbar(scatter, ticks=[0, 1], label='Made Tournament') \ncbar.set_ticklabels(['No', 'Yes']) \nplt.show()\n\n\n\nHere, I decided to plot each of our datapoints across the first two principal components, and then color them by whether or not a team ended up making the tournament. As you can see, the first principal component does most of the heavy lifting, as the X-Axis does a much better job at classifying the two. Further, since principal component one has the most variance, the range of values is wider, as we can see by the scales on the axes.\n\n\nCode\nweights_first_pc = components_df.iloc[0]\n\n# Sort the weights in descending order\nsorted_weights = weights_first_pc.sort_values(ascending=False)\n\n# Plot the bar chart with adjusted figure size and rotated x-axis labels\nplt.figure(figsize=(7, 4))  # Adjust the figure size as needed\nax = sorted_weights.plot(kind='bar', color='blue', alpha=0.7)\nplt.xlabel('Feature')\nplt.ylabel('Weight in First Principal Component')\nplt.title('Weights of Features in First Principal Component')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=8)  # Set fontsize for x-axis labels\nplt.tight_layout()  # Adjust layout for better spacing\nplt.show()\n\n\n\nIn this graph, I decided to visualize how much weight each of our features has in the first principal component. Clearly, the YearOffenseRank and YearDefenseRanks have a strong positive linear combination value. This is because a more optimal value for these would be lower (rank 1 is the best rank). From this, we can see that most of the values on the left side are our defensive metrics. Stronger defensive metrics are also classified by lower values. The inverse is true on the other side of our graph. For values that the best teams seek to maximize (Wins, BARTHAG, offensive metrics), they are given a negative linear combination value. From this, we can deduce that our best teams will have a low value on the first principal component. This tracks with our scatter plot above, where the teams that made the tournament were overwhelmingly on the left side of our plot.\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\nFor my usage of t-SNE, I decided to remove the “Made Tournament” column from my dataset. I wanted to see if the remaining features captured within the BartTorvik data naturally sorted themselves into tournament and non-tournament teams. To do this, I used three different perplexity scores. A perplexity score measures the number of neighbors each data point considers during the dimensionality reduction process. A low perplexity may lead to finer grained clusters, while a higher perplexity may lead to analyzing the data on a more global scale. For this reason, I selected one high perplexity (50), one low perplexity (5), and one medium perplexity (30). My initial assumption is that the medium perplexity will create a Goldilocks situation where it hopefully fits just right.\n\n\nCode\nperplexities = [5, 30, 50]\n\nbart_numeric = bart.copy()\nbart_numeric['Made_NCAA_Tournament'] = bart_numeric['Made_Tournament'].map({1: 'Yes', 0: 'No'})\n\nfor perplexity in perplexities:\n    tsne = TSNE(n_components=3, perplexity=perplexity, random_state=42)\n    bart_tsne = tsne.fit_transform(bart_numeric.drop(columns=['Made_NCAA_Tournament']))\n\n    fig = px.scatter_3d(x=bart_tsne[:, 0], y=bart_tsne[:, 1], z=bart_tsne[:, 2], color=bart_numeric['Made_NCAA_Tournament'],\n                        title=f'3D t-SNE Plot (Perplexity={perplexity})',\n                        color_discrete_map={'Yes': 'red', 'No': 'blue'},\n                        labels={'color': 'Made_NCAA_Tournament'})\n    \n    fig.update_layout(scene=dict(xaxis_title='Dimension 1', yaxis_title='Dimension 2', zaxis_title='Dimension 3')) \n    fig.show()\n\n\nC:\\Users\\davog\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning:\n\nThe default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n\nC:\\Users\\davog\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning:\n\nThe default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n\nC:\\Users\\davog\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning:\n\nThe default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n\nC:\\Users\\davog\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning:\n\nThe default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n\nC:\\Users\\davog\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning:\n\nThe default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n\nC:\\Users\\davog\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning:\n\nThe default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n\n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nFrom these three plots, we can begin to deduce a pattern. At the perplexity score of 5, our points are rather scattered. There are early semblances of a stricter form or pattern to our data, but it’s harder to visualize. At a score of 30, our data takes on a shape with stricter form. It more clearly shows a separation between teams. The majority of teams colored as having made the tournament score high on the x-axis and z-axis. While other teams score high on one of these two axes, teams with high values in both overwhelmingly tended to make the tournament. At the score of 50, this shape was even more tight, best captured by the shape seen below.\n\nFrom this, we can see a clear distinction between teams who made the tournament (red) and teams who did not (blue).\n\n\nConclusion\nFor both methods of dimensionality reduction, we were able to pretty accurately map whether or not a team made the tournament to the reduced dimensions. While dimensionality reduction isn’t a classification technique, this goes to show that the variance of the other features in our data, when captured, does a a great job at determining which teams end up making the tournament.\nFor PCA, we needed 8 components to explain 80% of the variance within our data. The first component did a lot of the heavy lifting, with 35% of the variance coming from it alone. Once again, it seems like our usual suspects (BARTHAG, WAB, W, and the offensive/defensive efficiency metrics) are the features that determine the most about a team.\nFor t-SNE, we tried three different perplexities on our data. The perplexity scores of 30 and 50 show rather similar shapes, both of which seem to cluster the majority of top teams together. It is interesting to see the different patterns in the data that were picked up on.\nGoing forward, I’m interested to see if dedicated clustering techniques do a similarly good job of defining the distinctions between teams in my dataset."
  },
  {
    "objectID": "data_gathering/data_gathering.html",
    "href": "data_gathering/data_gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Data Gathering\n\nAccessing the ncaahoopR R package\nFor my data sources, there were a few places I wanted to start.\nThe first is the ncaahoopR package. The package scrapes the ESPN.com website to generate different data sets based on publicly available NCAA basketball data. ncaahoopR is easy to install and use, and allows me to generate data from any games tracked on ESPN.\nFurther information about the package can be found here.\nHere is an example of me exploring this package. I used the “get_pbp” function to collect a data frame of all play-by-play data for Creighton (my favorite team) from the 2022-23 season. Each time the function is called, the package scrapes all game data from the entire season, which takes a few minutes to collect.\n\n\nCode\nlibrary(ncaahoopR)\n\ncreighton &lt;- get_pbp(\"Creighton\", \"2022-23\")\n\nwrite.csv(creighton, file = \"../data/creighton_2022_23.csv\", row.names = FALSE)\n\n\n\n\nCode\nhead(creighton[, 1:5], 10)\n\n\n     game_id       date                 home      away play_id\n1  401485099 2022-11-07 St. Thomas-Minnesota Creighton       1\n2  401485099 2022-11-07 St. Thomas-Minnesota Creighton       2\n3  401485099 2022-11-07 St. Thomas-Minnesota Creighton       3\n4  401485099 2022-11-07 St. Thomas-Minnesota Creighton       4\n5  401485099 2022-11-07 St. Thomas-Minnesota Creighton       5\n6  401485099 2022-11-07 St. Thomas-Minnesota Creighton       6\n7  401485099 2022-11-07 St. Thomas-Minnesota Creighton       7\n8  401485099 2022-11-07 St. Thomas-Minnesota Creighton       8\n9  401485099 2022-11-07 St. Thomas-Minnesota Creighton       9\n10 401485099 2022-11-07 St. Thomas-Minnesota Creighton      10\n\n\n\n\nCode\ncat(\"Number of rows:\", nrow(creighton), \"\\n\")\n\n\nNumber of rows: 11128 \n\n\nCode\ncat(\"Number of columns:\", ncol(creighton), \"\\n\")\n\n\nNumber of columns: 39 \n\n\nObviously, this is a massive data set with lots of potential insights. One thing that initially stood out to me are the two win probability columns. The first, “win_prob”, is a snapshot of the home team’s win probability during a specific play after factoring in pre-game point spread. The other column, “naive_win_prob”, doesn’t use pre-game point spread to determine win probability.\n\n\nCode\nlibrary(dplyr)\n\ncreighton_change &lt;- creighton %&gt;% filter(scoring_play == TRUE,\n  win_prob &gt;= lag(win_prob, default = first(win_prob)) + .2 |     \n  win_prob &lt;= lag(win_prob, default = first(win_prob)) - .2)\n\nhead(creighton_change[, 1:5], 10)\n\n\n     game_id       date         home          away play_id\n1  401485141 2022-11-17 UC Riverside     Creighton       1\n2  401486690 2022-11-22     Arkansas     Creighton     299\n3  401486690 2022-11-22     Arkansas     Creighton     304\n4  401486690 2022-11-22     Arkansas     Creighton     309\n5  401486690 2022-11-22     Arkansas     Creighton     336\n6  401483381 2022-12-04    Creighton      Nebraska       1\n7  401485177 2022-12-10    Creighton           BYU     383\n8  401485177 2022-12-10    Creighton           BYU     384\n9  401485177 2022-12-10    Creighton           BYU     388\n10 401483440 2022-12-12    Creighton Arizona State     318\n\n\nIn the filtered data frame above, I looked to see how many scoring plays there were throughout the season that impacted Creighton’s win percentage by +/- 20%. Of the 11444 plays recorded during the season, only 32 fit such criteria.\n\n\nAccessing Barttorvik’s NCAA Data using Python\nBarttorvik is one of the two major sites for advanced college basketball team statistics. They track dozens of advanced metrics, creating efficiency and team rank methodology with the goal of determining the best teams. College basketball had 363 NCAA Division 1 Men’s programs in 2023. Since there are so many teams, it becomes extremely difficult to effectively evaluate teams head to head.\nThis site looks to make comparisons between teams and across years much easier. Luckily, they make all of their data sets free and easily accessible here.\nI started by using the requests package in Python to import the .csv of last year’s data. It contains 45 columns and 363 rows (one for each team). As the project goes on, I will likely incorporate previous years’ data as well in order to analyze trends from year to year.\n\n\nCode\n#Imports\nimport pandas as pd\nimport numpy as np\nimport requests\nimport csv\n\n\n\n\nCode\ncsv_url = \"http://barttorvik.com/2023_team_results.csv\"\nlocal_filename = \"barttorvik_2023.csv\"\n\nresponse = requests.get(csv_url)\n\nif response.status_code == 200:\n    with open(local_filename, 'wb') as outfile:\n        outfile.write(response.content)\nelse:\n    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n\n\n\n\nCode\nbarttorvik_df = pd.read_csv(\"barttorvik_2023.csv\")\n\nbarttorvik_df.columns = ['Team', 'Conf', 'Record', 'AdjOE', 'OERank', 'AdjDE', 'DERank', 'Barthag', 'Rank', 'ProjW', 'ProjL', 'ProConW', 'ProConL', 'ConRec', 'SOS', 'NCSOS', 'Consos', 'ProjSOS', 'ProjNonconSOS', 'ProjConSOS', 'EliteSOS', 'EliteNonconSOS', 'OppOE', 'OppDE', 'OppProjOE', 'OppProjDE', 'ConAdjOE','ConAdjDE', 'QualO', 'QualD', 'QualBarthag', 'QualGames', 'FUN', 'ConPF','ConPA', 'ConPoss', 'ConOE', 'ConDE', 'ConSOSRemain', 'ConfWinPct', 'WAB', 'WABRk', 'FunRk', 'AdjT']\n\nprint(barttorvik_df.head())\n\n\n          Team  Conf Record       AdjOE  ...        WAB  WABRk  FunRk       AdjT\n1  Connecticut    BE   31-8  121.478653  ...   5.033129     13    280  66.727894\n2      Alabama   SEC   31-6  116.081342  ...  10.193101      1     89  72.551428\n3      Houston  Amer   33-4  117.307101  ...   8.046128      5    111  63.359244\n4         UCLA   P12   31-6  114.914658  ...   8.402795      4    152  66.071648\n5    Tennessee   SEC  25-11  111.211495  ...   2.809739     25    349  65.146696\n\n[5 rows x 44 columns]\n\n\nCode\n\nbarttorvik_df.to_csv(\"../data/barttorvik_2023.csv\", index= False)\n\n\nWhen importing the 2023 data, I realized that the column names imported incorrectly. I went through and renamed each column in order to ensure the data was properly represented and easy to access.\n\n\nCode\nfull_barttorvik = pd.read_csv(\"../data/cbb.csv\")\n\nvalues_to_change = [\"Champions\", \"2ND\", \"F4\", \"E8\", \"S16\", \"R32\", \"R64\", \"N/A\"]\nnew_values = [7, 6, 5, 4, 3, 2, 1, 0]\n\nfull_barttorvik[\"POSTSEASON\"] = full_barttorvik[\"POSTSEASON\"].replace(values_to_change, new_values)\n\nfull_barttorvik[\"SEED\"].fillna(17, inplace=True)\nfull_barttorvik[\"POSTSEASON\"].fillna(0, inplace=True)\n\nfor year in range(2013, 2024):\n    filtered_data = full_barttorvik[full_barttorvik['YEAR'] == year]\n    full_barttorvik.loc[full_barttorvik['YEAR'] == year, 'YearOffenseRank'] = filtered_data['ADJOE'].rank(ascending=False).astype(int)\n    full_barttorvik.loc[full_barttorvik['YEAR'] == year, 'YearDefenseRank'] = filtered_data['ADJDE'].rank(ascending=True).astype(int)\n\nfull_barttorvik['TotalOffenseRank'] = full_barttorvik['ADJOE'].rank(ascending=False).astype(int)\nfull_barttorvik['TotalDefenseRank'] = full_barttorvik['ADJDE'].rank(ascending=True).astype(int)\n\nfull_barttorvik[\"POSTSEASON\"] = pd.to_numeric(full_barttorvik[\"POSTSEASON\"], errors='coerce')\nfull_barttorvik[\"POSTSEASON\"].replace('', pd.NA, inplace=True)\nfull_barttorvik[\"Made_Tournament\"] = full_barttorvik[\"POSTSEASON\"].apply(lambda x: 1 if 1&lt;= x &lt;= 7 else 0)\n\nfull_barttorvik.to_csv(\"../data/cbb.csv\", index=False)\n\n\nI also sourced a dataset that contains the same information as the 2023 BartTorvik data, but contains all team records since 2013. This more expansive dataset will be helpful in creating a more thorough analysis. With this dataset, I edited the POSTSEASON column to reflect how many NCAA Tournament rounds a team experienced. A team that didn’t make the tournament would receive a value of 0, while a team that won the National Championship by winning 6 tournament games would receive a value of 7.\nI also ranked each team’s offensive and defensive efficiencies by year as this was something this dataset was lacking. I then resaved it for later use.\n\n\nAccessing KenPom’s NCAA Data Using Python\nThe other major college basketball site for advanced team statistics is KenPom. KenPom has their own ranking methodology, similar to Barttorvik. While many of their advanced stats are similar, they are markedly different and each worth exploring in their own right. The site can be viewed here.\nWhile KenPom’s homepage is free to view, accessing their full data requires a $20 annual subscription. I have this subscription, which allows me to download the site’s data sets and import them into Pandas in a similar way to the Barttorvik data.\nAs a side note, there is a “hoopR” package that is supposedly able to pull from the KenPom website directly with subscription credentials. I spent a few hours trying to figure out how to get it to work and unfortunately have had no luck. I may try to revisit this method down the line. I included my initial exploration of the package below.\n\n\n[1] TRUE\n\n\n\n\nCode\nkenpom_df = pd.read_csv(\"../data/summary23.csv\")\n\nprint(kenpom_df.head())\n\n\n   Season           TeamName    Tempo  ...  RankAdjDE     AdjEM  RankAdjEM\n0    2023  Abilene Christian  69.8901  ...        224  -1.46922        193\n1    2023          Air Force  63.6234  ...        164   1.25207        147\n2    2023              Akron  66.9744  ...        138   6.15562        105\n3    2023            Alabama  72.7280  ...          3  27.28420          4\n4    2023        Alabama A&M  69.8428  ...        244 -11.62140        312\n\n[5 rows x 16 columns]\n\n\nKenPom only provides 16 different features for their main data set, but having the subscription provides access to individual team pages and data, allowing for more in-depth information."
  },
  {
    "objectID": "conclusion/conclusions.html#can-we-predict-which-teams-make-the-ncaa-tournament",
    "href": "conclusion/conclusions.html#can-we-predict-which-teams-make-the-ncaa-tournament",
    "title": "Conclusions",
    "section": "Can We Predict Which Teams Make The NCAA Tournament?",
    "text": "Can We Predict Which Teams Make The NCAA Tournament?\nGoing into this project, my goal was to try to figure out how well we could predict which teams would be among the 68 selected each year to participate in the NCAA Men’s Basketball Tournament. So, how did I do? I’ll recap my findings from each aspect of this project and then leave you to decide.\n\nData Gathering\nDue to the different requirements of the project, I used a few different sources of data. The first was the usage of BartTorvik’s historical college basketball dataset. This dataset contained every division 1 team since 2013 (besides 2020), giving us plenty of information about their metrics and performance. All of this information is readily available and easily downloadable from BartTorvik’s website.\nNext, I sourced scraped text data from the ncaahoopR package. This allowed me to get play by play data for each of Creighton’s 37 games played in the 2022-23 season. While this data didn’t necessarily tie in to my overall research question, Creighton is my favorite team and it was cool to analyze.\nIf I could have got the hoopR package to work, I would have also incorporated KenPom data into my analysis. KenPom is very similar to BartTorvik in terms of metrics provided, so it would have been interesting to see which site does a better job.\n\n\nData Cleaning\nLuckily for me, there wasn’t a whole lot to clean. The datasets imported relatively cleanly so outside of choosing which columns I wanted to keep and renaming a few of them, there wasn’t much for me to do. This is probably the step I was dreading the most so I’m immensely grateful I could get my hands on clean data.\n\n\nExploratory Data Analysis\nHere, I started to dig a bit deeper into the data provided. My favorite visualization produced was probably the percentage of teams from each conference who have made the tournament over the last decade. While some conferences only get their auto qualifier each year, the Big 12 has sent 65% of their teams over the last decade to the tournament. The correlation matrices also provided good insight and reaffirmed that BartTorvik’s in-house BARTTHAG rating has a strong relationship to tournament performance.\n\n\nNaive Bayes/Dimensionality Reduction/Clustering\nIn these tabs, I finally started the modeling process. For Naive Bayes, the optimal feature set for tournament classification included Wins Above Bubble, BARTTHAG rating, and total wins. These columns, as we ended up seeing throughout the project, were kind of a cheat code in terms of creating accurate models. However, when attempting Naive Bayes on my Creighton text data, I hit a wall. Even when finding the optimal text feature set, the classifier would always guess that Creighton wouldn’t score on each play. This taught me a lot about the limitations of modeling, especially considering I didn’t have the computing power to iterate over a huge feature set. Further, it showed that accuracy score was not always a great metric. Our Creighton Naive Bayes model had &gt;80% accuracy, but that was only because Creighton scored on very few of the plays.\nIn the dimensionality reduction tab, it was cool to see the linear combinations that explained the most variance. PCA’s results seemed fairly intuitive, and when mapping our classifier of tournament teams onto the principal components, there was clear overlap.\nFor clustering, each method resulted in selecting 2 optimal clusters. This was interesting to me, but allowed for me to see if the 2 clusters from each method had good overlap with one another as well as the “Made Tournament” classifier. From the K-Means method, we were able to see that nearly all tournament teams in the dataset (618/640) were clustered together.\n\n\nDecision Trees\nLastly, I produced two different decision trees from my dataset. Again, I set the classifier to be making the tournament, and sought to see how well a decision tree could create this classification prediction. On the first iteration, it performed incredibly well, despite deciding on its classification after a single split. The common theme throughout the project was that WAB and BARTTHAG were too powerful in terms of classification and prevented us from seeing the underlying traits that make a team successful. By simply splitting teams by if they had more or less than 0.15 Wins Above the Bubble, our first tree achieved 93% accuracy.\nTo refine this process, I removed those columns as to give the algorithm more of a challenge. On the second go around, the max_depth parameter was increased to 4 and from this we were able to discern information that was probably more interesting. Offensive Efficiency ranking seems to play a big role in a team’s success, but a high OE alone isn’t enough to give a team a tournament birth. Things like low turnover rates, high effective field goal percentage, and a great defense also contributed to this classification.\n\n\nNext Steps/Wrapping Up:\nWhile I didn’t uncover anything in this project that is going to change the world of college basketball, I had a lot of fun applying these different techniques to the dataset. For most of these modeling tasks, this was my first time attempting them, and by trying them on a dataset I’m familiar with and interested in, I felt as though I was able to learn a lot more. If I were to do it over again, I would probably remove all teams who qualified by winning their conference tournaments from the dataset. This would mean that our classification tasks would only focus on the 36 at-large qualifiers each year. I think it would lend itself to some interesting results as the teams that often lowered our accuracy scores were the automatic qualifiers from small conferences, who would have no shot at getting into the tournament without the auto bid. Maybe someday I’ll revisit this project with that idea in mind. Until then, I’m really happy with the way this turned out and am glad that I have something tangible to show for my first semester in the program. Day-to-day (or even week-to-week) I often struggle to feel the progression of my learning, but it’s really cool to look back and know that even 15 weeks ago, I would have had almost no clue how to do most of this project. I’m thankful for a great first semester and am excited to continue learning in the spring!\nBest,\nDominic"
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "About Me",
    "section": "",
    "text": "Dominic Vogel is a first-year student in Georgetown’s MS Data Science and Analytics program. He is enrolled in the accelerated track, taking classes concurrently with undergraduate coursework. For undergrad, Dominic is enrolled in Georgetown’s McDonough School of Business, double-majoring in Operations & Analytics and Finance. Outside of the classroom, Dominic is the President of Georgetown’s Transfer Council, helping foster a transfer student community on campus. He also runs Georgetown Survivor, hosting a mock season on campus this fall. Dominic is also a big Minnesota Twins fan, tries to listen to a new album each day, and knows far too much about Pokemon.\n\nThis is a picture of me from when I went to Mt. Rainier this summer.\n\n\nHometown: Minneapolis, MN\nFavorite Album of 2023: Struggler- Genesis Owusu\nFavorite Pokemon: Torterra\nFavorite Sports Teams: Minnesota Twins, Minnesota Timberwolves, Creighton Bluejays\n\n\n\n\n2025: Georgetown University- MS Data Science and Analytics\n2024: Georgetown University- BS Operations & Analytics | Finance"
  },
  {
    "objectID": "about_me.html#about-dominic",
    "href": "about_me.html#about-dominic",
    "title": "About Me",
    "section": "",
    "text": "Dominic Vogel is a first-year student in Georgetown’s MS Data Science and Analytics program. He is enrolled in the accelerated track, taking classes concurrently with undergraduate coursework. For undergrad, Dominic is enrolled in Georgetown’s McDonough School of Business, double-majoring in Operations & Analytics and Finance. Outside of the classroom, Dominic is the President of Georgetown’s Transfer Council, helping foster a transfer student community on campus. He also runs Georgetown Survivor, hosting a mock season on campus this fall. Dominic is also a big Minnesota Twins fan, tries to listen to a new album each day, and knows far too much about Pokemon.\n\nThis is a picture of me from when I went to Mt. Rainier this summer.\n\n\nHometown: Minneapolis, MN\nFavorite Album of 2023: Struggler- Genesis Owusu\nFavorite Pokemon: Torterra\nFavorite Sports Teams: Minnesota Twins, Minnesota Timberwolves, Creighton Bluejays\n\n\n\n\n2025: Georgetown University- MS Data Science and Analytics\n2024: Georgetown University- BS Operations & Analytics | Finance"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Aftering incorporating dimensionality reduction on the last page, it is now time to look further into clusters that appear within the data. While the way I have been looking at the data to this point has been rather binary (a team either made or missed the tournament), there are many possible ways to cluster the data. There is no single optimal clustering selection method, so I plan to try out a few different ones in this section. I will be using the same feature set of my NCAA data that I used on the dimensionality reduction page. That data was saved from before and will be reimported below.\n\n\n\n\nCode\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport sklearn\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nbart = pd.read_csv('../data/bart_dimensionality.csv')\nremove_target = ['Made_Tournament']\nbart_target = bart.copy()\nbart.drop(columns= remove_target, inplace=True)\n\n\nAs always, pandas and the plotting libraries need to be imported. sklearn will be used to implement our different clustering methods. Lastly, the scipy packag will be used to visualize our agglomerative clustering results. This time, we will be using the feature set that was used for our dimensionality reduction tab.\n\n\n\n\n\n\nK-Means clustering is an unsupervised clustering method that aims to group data points into ‘k’ amount of clusters. In this context, ‘k’ represents a user specified parameter, meaning that it’s ultimately up to me to decide how many clusters I want to incorporate. To decide on this value ‘k’, there are a few different methods that can be used, such as the elbow method and analyzing silhouette scores.\nK-Means clustering seeks to optimize the sum of the squared distances between data points and their cluster mean. Each cluster starts with a randomly initialized centroid. Through iteration, new data points are assigned to the closest centroid and the centroids are updated based off of the mean of their assigned points. Eventually, the centroids converge and stabilize, resulting in the finished product.\nK-Means is efficient computationally, which helps when there is a large feature and dataset. It also is easily interpretable, allowing us to better discover patterns within the data.\nTo start, I once again standardized the data. From there, I decided to test out a range of 2 to 10 clusters to determine which will be best for our analysis.\n\n\nCode\nwarnings.filterwarnings('ignore')\n\nscaler = StandardScaler()\nbart_standardized = scaler.fit_transform(bart)\n\nclusters = list(range(2, 11))\ninertia = []\ndistortion_val = []\nsilhouette_scores = []\n\nfor c in clusters:\n    kmeans = KMeans(n_clusters=c)\n    kmeans.fit(bart_standardized)\n    distortion = sum(np.min(kmeans.transform(bart_standardized), axis=1)) / bart_standardized.shape[0]\n    distortion_val.append(distortion)\n    inertia.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(bart_standardized, kmeans.labels_))\n\nresults_df = pd.DataFrame({\"Cluster\": clusters, \"Distortion\": distortion_val, \"Inertia\": inertia, 'Silhouette Score': silhouette_scores})\n\nprint(results_df)\n\n\n   Cluster  Distortion       Inertia  Silhouette Score\n0        2    3.796191  53547.050447          0.204415\n1        3    3.630575  48901.884040          0.141696\n2        4    3.512207  45674.398093          0.117894\n3        5    3.433692  43640.958459          0.101701\n4        6    3.377312  42204.800408          0.090066\n5        7    3.330546  41002.527109          0.088327\n6        8    3.291205  40017.516560          0.085486\n7        9    3.256279  39190.474780          0.082008\n8       10    3.231467  38512.970293          0.080463\n\n\nFrom this, we are able to view our distortion, inertia, and silhouette scores for each of our nine attempted cluster sizes. The results are plotted and analyzed below.\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.plot(results_df['Cluster'], results_df['Distortion'], marker='o')\nplt.title('Distortion')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Distortion')\nplt.show()\n\n\n\nAs seen above, we plotted the distortion score for each number of clusters. Usually, the goal is to identify an “elbow point” within our data, where the rate of decrease sharply changes. To me, it looks like the elbow point might be at 4, but it isn’t particularly convincing. Let’s analyze further and see what happens when we do the same plot for inertia.\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.plot(results_df['Cluster'], results_df['Inertia'], marker='o', color='orange')\nplt.title('Inertia')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n\n\nWhen looking at inertia, we once again look for this elbow point. To me, it seems like 4 is our best guess at where the elbow point would lie, but the rate of change is relatively constant throughout. Because of this, I want to lastly plot the silhouette scores to determine my optimal number of clusters.\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.plot(results_df['Cluster'], results_df['Silhouette Score'], marker='o', color='green')\nplt.title('Silhouette Score')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\nAfter plotting the silhouette score across different numbers of clusters, we notice something interesting. The silhouette score is a metric that quantifies how well separated the clusters are. Ideally, we want to choose the number of clusters that maximize this silhouette score. From our plot, it becomes clear that 2 clusters results in the highest silhouette score by a considerable margin. Because of this, we will choose 2 clusters for our K-Means analysis.\n\n\nCode\npca_components = pd.read_csv(\"../data/pca_components.csv\")\n\npca = PCA(n_components=3)\nbart_pca = pca.fit_transform(bart_standardized)\ncomponents = pca.components_\n\nkmean = KMeans(n_clusters=2, random_state=101)\nkmean.fit(bart_standardized)\nlabels = kmean.labels_\n\npca_df = pd.DataFrame(data=bart_pca, columns=['PCA1', 'PCA2', 'PCA3'])\npca_df['Cluster'] = labels\n\nplt.figure(figsize=(10, 6))\n\nsns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=pca_df, palette='viridis', edgecolor='w', s=80)\nplt.title('K-Means Clusters in PCA Space (First Two Components)')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.show()\n\n\n\nAfter running K-Means clustering with a ‘k’ of 2, I decided to plot our clusters against the first two principal components. When doing PCA on the last tab, I saved a file with the linear combinations for each of the first two principal components. Now, when applying those linear combinations to each data point and classifying them with their K-Means cluster number, we can see this result. The first principal component seems to do a pretty efficient job of clustering our data, as the clusters are almost entirely divided down the x-axis.\n\n\nCode\npca_df['Cluster'] = pca_df['Cluster'].astype(str)\n\nfig = px.scatter_3d(pca_df, x='PCA1', y='PCA2', z='PCA3', color='Cluster', symbol='Cluster', opacity=0.7, size_max=10, title='3D Scatter Plot of PC1, PC2, and PC3 with Cluster Labels', labels={'0': 'Principal Component 1 (PC1)', '1': 'Principal Component 2 (PC2)','2': 'Principal Component 3 (PC3)', 'color': 'Cluster'}, color_discrete_sequence=['blue', 'green'])\n\nfig.update_layout(scene=dict(annotations=[dict(x=0, y=0, z=0, text='Cluster', showarrow=False)]))\nfig.update_layout(scene=dict(xaxis=dict(title_font=dict(size=16)),\n                             yaxis=dict(title_font=dict(size=16)),\n                             zaxis=dict(title_font=dict(size=16))))\n\nfig.show()\n\n\n\n                                                \n\n\nHere, I decided to include the third principal component and run the same analysis. Obviously, the first principal component remains dominant in determining cluster, but it is still cool to visualize the data in this way.\n\n\nCode\noptimal_k = 2\n\nkmeans = KMeans(n_clusters=optimal_k, random_state=101)\nlabels = kmeans.fit_predict(bart_standardized)\n\nbart_with_clusters = bart_target.copy()\nbart_with_clusters['Cluster'] = labels\nbart_with_clusters['Cluster'] = 1 - bart_with_clusters['Cluster'] \n\nalignment_count = bart_with_clusters.groupby(['Cluster', 'Made_Tournament']).size().reset_index(name='Count')\n\nmisalignment_counts = alignment_count[alignment_count['Cluster'] == alignment_count['Made_Tournament']]\nalignment_counts = alignment_count[alignment_count['Cluster'] != alignment_count['Made_Tournament']]\n\nprint(\"Counts for alignment:\")\nprint(alignment_counts)\n\nprint(\"\\nCounts for misalignment:\")\nprint(misalignment_counts)\n\n\nCounts for alignment:\n   Cluster  Made_Tournament  Count\n1        0                1    618\n2        1                0   1812\n\nCounts for misalignment:\n   Cluster  Made_Tournament  Count\n0        0                0   1071\n3        1                1     22\n\n\nNext, I wanted to see how closely our clustering resembled our Made Tournament column. The results here are interesting. 618 out of the 640 tournament teams in our dataset ended up in the same cluster (cluster 0). However, that cluster also contained 1071 teams that didn’t make the tournament. This makes sense to me as there really is no hard cutoff for tournament caliber teams and ones that are on the outside looking in. Most years results are fluid, and the last 10 or so teams that make it in could easily be exchanged for the next 10 or so that barely miss the cut. Our clustering algorithm wasn’t given specific numbers of points to put in each cluster, and because of this, the size of each cluster looks relatively similar (1689 for cluster 0 and 1834 for cluster 1). Considering only ~18% of our dataset made the tournament, there is a misalignment in sheer size.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.bar(alignment_counts['Cluster'].astype(str) + '_' + alignment_counts['Made_Tournament'].astype(str), alignment_counts['Count'], color='green', label='Alignment')\n\nplt.bar(misalignment_counts['Cluster'].astype(str) + '_' + misalignment_counts['Made_Tournament'].astype(str), misalignment_counts['Count'], color='red', label='Misalignment')\n\nplt.xlabel('Cluster_Made_Tournament')\nplt.ylabel('Count')\nplt.title('Alignment and Misalignment Counts')\nplt.legend()\nplt.show()\n\n\n\nThis is just a further visualization of what was discussed above. This bar chart does a good job of conveying that there were very few tournament teams (22 out of 640, ~3.4%) that were classified as cluster 1.\n\n\n\n\n\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is another clustering algorithm that seeks to find patterns of similarities within the data. DBSCAN differs from K-Means as it doesn’t require us to specify the number of clusters we want in advance.\nDBSCAN categorizes points into one of three categories: core points, border points, and noise points. Core points sit at, as the name suggests, the core of each cluster. They have a number of neighbors within a specified radius. Border points are in proximity to these core points but do not share the same density of points around them as core points do. Noise points are isolated points that aren’t classified as being part of any particular cluster.\nDBSCAN, because of these noise points, does a great job at handling outliers effectively. Since points do not have to be forced into a particular cluster, the clusters that are formed are often more compact. DBSCAN has two parameters- epsilon and minimum samples. Epsilon determines how far apart two points can be to remain in the same cluster. Minimum samples determines how many points need to be in a cluster for it to be designated as its own cluster.\nTo start, I looped through a range of epsilon values and minimum samples to determine which parameters led to the optimal silhouette score.\n\n\nCode\nbest_scores = []\nepsilons = []\nbest_clusters = []\n\neps_range = [i / 10 for i in range(25, 45)] \nmin_samples_range = range(2, 10)\n\nfor i in eps_range:\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    for j in min_samples_range:\n        model = DBSCAN(eps=i, min_samples=j)\n        predictions = model.fit_predict(bart_standardized)\n        num_clusters = len(set(predictions)) - (1 if -1 in predictions else 0)\n        if num_clusters &gt; 1: \n            score = silhouette_score(bart_standardized, predictions)\n            if score &gt; max_score:\n                max_score = score\n                best_cluster = num_clusters\n                best_eps = i\n\n    best_scores.append(max_score)\n    best_clusters.append(best_cluster)\n    epsilons.append(best_eps)\n\ndb_df = pd.DataFrame({'Epsilons': epsilons, 'Best_Clusters': best_clusters, 'Silhouette_Score': best_scores})\ndb_df = db_df[(db_df['Best_Clusters'] != -1) & (db_df['Silhouette_Score'] != -1)]\n\nplt.figure()\nsns.lineplot(data=db_df, x=\"Best_Clusters\", y=\"Silhouette_Score\")\nplt.title('Silhouette Score vs. Best Clusters (DBSCAN)')\nplt.xlabel('Best Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\nFrom these results, it once again seems like our silhouette score is maximized with 2 clusters. Looping through only the epsilon parameters of 2.5 to 4.5 seemed like a weird choice, but that range was found via trial and error. If the epsilon parameters are any lower, the silhouette scores quickly become negative, as our dataset has enough features and variance that most points become classified as outliers. For this reason, we need to increase the maximum distance allowed for two points, resulting in this analysis.\n\n\nCode\nprint(db_df.sort_values(by=\"Silhouette_Score\", ascending=False).head())\n\n\n    Epsilons  Best_Clusters  Silhouette_Score\n16       4.1              2          0.281549\n17       4.2              2          0.281549\n15       4.0              2          0.258917\n13       3.8              2          0.209236\n12       3.7              2          0.206638\n\n\nFrom this, we can see that all five of the maximum silhouette scores came from 2 clusters.\n\n\nCode\nmax_silhouette_score = db_df['Silhouette_Score'].max()\nmax_silhouette_row_index = db_df['Silhouette_Score'].idxmax()\nmax_silhouette_row = db_df.loc[max_silhouette_row_index]\n\nprint(\"Maximum Silhouette Score:\", max_silhouette_score)\n\n\nMaximum Silhouette Score: 0.28154948017583054\n\n\nHere, our maximum silhouette score via the DBSCAN method is 0.28.\n\n\n\n\n\n\nHierarchical clustering is a clustering method that seeks to group data points into a hierarchical structure based on similarity. Hierarchical clustering does not require a predefined number of clusters, but instead produces a dendogram which shows a visual representation of the data’s relationship amongst itself.\nHierarchical clustering begins by treating each datapoint as its own, individual cluster. By iterating through the process, the algorithm begins to merge the most similar of these clusters, continuing to do so until there is one singular cluster with all of the datapoints. Inputs can vary based on different linkage and affinity criteria. To start, I will use Ward linkage, which seeks to minimize the variance between clusters. The affinity parameter determines how this distance is measured. I will be using Euclidean distance.\nHierarchical clustering is especially beneficial at revealing nested structures that may be present within the data. Because of this, it is easy to visualize different clusters at different scales. Depending on how many clusters one might want to use, it’s easy to see the divisions of these clusters when looking at the dendogram.\n\n\nCode\nnum_clusters = 2\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters= num_clusters, affinity='euclidean', linkage='ward') \nlabels = hierarchical_cluster.fit_predict(bart_standardized)\nprint(\"Cluster Labels total:\")\nprint(list(set(labels)))\n\n\nCluster Labels total:\n[0, 1]\n\n\nTo begin, I input the number of clusters as 2. This number of clusters will be used to label the leaves of the dendogram generated below.\n\n\nCode\nlinkage_matrix = linkage(bart_standardized, method='ward')\n\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, orientation='top', labels=labels, distance_sort='ascending', show_leaf_counts=True)\nplt.show()\n\n\n\nFrom the dendogram, we can see the hierarchical distinction between our two clusters. Cluster 1 looks to be considerably larger than cluster 0.\n\n\nCode\nsilhouette_scores = []\n\nfor n_clusters in range(2, 11):\n    hierarchical_cluster = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n    labels = hierarchical_cluster.fit_predict(bart_standardized)\n\n    if len(set(labels)) &gt; 1:\n        score = silhouette_score(bart_standardized, labels)\n        silhouette_scores.append({\"Number of Clusters\": n_clusters, \"Silhouette Score\": score})\n\nsilhouette_df = pd.DataFrame(silhouette_scores)\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=silhouette_df, x=\"Number of Clusters\", y=\"Silhouette Score\", marker=\"o\")\nplt.title('Silhouette Score vs. Number of Clusters (Agglomerative Clustering)')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\nWhile the first dendogram assumed two clusters was the ideal (as it was for K-Means and DBSCAN), I wanted to determine from the hierarchical silhouette score method if it determined 2 clusters was ideal for my dataset as well. After looping through the range of 2 to 10 clusters, it becomes pretty clear from the plot that the silhouette score is maximized at a cluster size of 2.\n\n\n\nIf this page taught us one thing, it would be that there seem to be two distinct clusters within our dataset. Each method disagrees to an extent about which specific points are in which cluster, but the size of two being optimal remains constant. While clustering is not a supervised learning method, I did want to see how closely the clusters aligned with which teams actually made the tournament. From the K-Means method, we were able to determine that our clustering does a good job at determining which teams have a shot to make the tournament. However, the boundary of where to separate the cluster is the big distinction. When splitting teams into 2 clusters, it seems clear that one of them is going to contain the majority of the more successful teams. However, that cluster seems to blend many more teams in with them too, implying that the boundary between tournament quality team and non-tournament quality team is blurry."
  },
  {
    "objectID": "clustering/clustering.html#k-means-clustering",
    "href": "clustering/clustering.html#k-means-clustering",
    "title": "Clustering",
    "section": "",
    "text": "K-Means clustering is an unsupervised clustering method that aims to group data points into ‘k’ amount of clusters. In this context, ‘k’ represents a user specified parameter, meaning that it’s ultimately up to me to decide how many clusters I want to incorporate. To decide on this value ‘k’, there are a few different methods that can be used, such as the elbow method and analyzing silhouette scores.\nK-Means clustering seeks to optimize the sum of the squared distances between data points and their cluster mean. Each cluster starts with a randomly initialized centroid. Through iteration, new data points are assigned to the closest centroid and the centroids are updated based off of the mean of their assigned points. Eventually, the centroids converge and stabilize, resulting in the finished product.\nK-Means is efficient computationally, which helps when there is a large feature and dataset. It also is easily interpretable, allowing us to better discover patterns within the data.\nTo start, I once again standardized the data. From there, I decided to test out a range of 2 to 10 clusters to determine which will be best for our analysis.\n\n\nCode\nwarnings.filterwarnings('ignore')\n\nscaler = StandardScaler()\nbart_standardized = scaler.fit_transform(bart)\n\nclusters = list(range(2, 11))\ninertia = []\ndistortion_val = []\nsilhouette_scores = []\n\nfor c in clusters:\n    kmeans = KMeans(n_clusters=c)\n    kmeans.fit(bart_standardized)\n    distortion = sum(np.min(kmeans.transform(bart_standardized), axis=1)) / bart_standardized.shape[0]\n    distortion_val.append(distortion)\n    inertia.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(bart_standardized, kmeans.labels_))\n\nresults_df = pd.DataFrame({\"Cluster\": clusters, \"Distortion\": distortion_val, \"Inertia\": inertia, 'Silhouette Score': silhouette_scores})\n\nprint(results_df)\n\n\n   Cluster  Distortion       Inertia  Silhouette Score\n0        2    3.796191  53547.050447          0.204415\n1        3    3.630575  48901.884040          0.141696\n2        4    3.512207  45674.398093          0.117894\n3        5    3.433692  43640.958459          0.101701\n4        6    3.377312  42204.800408          0.090066\n5        7    3.330546  41002.527109          0.088327\n6        8    3.291205  40017.516560          0.085486\n7        9    3.256279  39190.474780          0.082008\n8       10    3.231467  38512.970293          0.080463\n\n\nFrom this, we are able to view our distortion, inertia, and silhouette scores for each of our nine attempted cluster sizes. The results are plotted and analyzed below.\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.plot(results_df['Cluster'], results_df['Distortion'], marker='o')\nplt.title('Distortion')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Distortion')\nplt.show()\n\n\n\nAs seen above, we plotted the distortion score for each number of clusters. Usually, the goal is to identify an “elbow point” within our data, where the rate of decrease sharply changes. To me, it looks like the elbow point might be at 4, but it isn’t particularly convincing. Let’s analyze further and see what happens when we do the same plot for inertia.\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.plot(results_df['Cluster'], results_df['Inertia'], marker='o', color='orange')\nplt.title('Inertia')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n\n\nWhen looking at inertia, we once again look for this elbow point. To me, it seems like 4 is our best guess at where the elbow point would lie, but the rate of change is relatively constant throughout. Because of this, I want to lastly plot the silhouette scores to determine my optimal number of clusters.\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.plot(results_df['Cluster'], results_df['Silhouette Score'], marker='o', color='green')\nplt.title('Silhouette Score')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\nAfter plotting the silhouette score across different numbers of clusters, we notice something interesting. The silhouette score is a metric that quantifies how well separated the clusters are. Ideally, we want to choose the number of clusters that maximize this silhouette score. From our plot, it becomes clear that 2 clusters results in the highest silhouette score by a considerable margin. Because of this, we will choose 2 clusters for our K-Means analysis.\n\n\nCode\npca_components = pd.read_csv(\"../data/pca_components.csv\")\n\npca = PCA(n_components=3)\nbart_pca = pca.fit_transform(bart_standardized)\ncomponents = pca.components_\n\nkmean = KMeans(n_clusters=2, random_state=101)\nkmean.fit(bart_standardized)\nlabels = kmean.labels_\n\npca_df = pd.DataFrame(data=bart_pca, columns=['PCA1', 'PCA2', 'PCA3'])\npca_df['Cluster'] = labels\n\nplt.figure(figsize=(10, 6))\n\nsns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=pca_df, palette='viridis', edgecolor='w', s=80)\nplt.title('K-Means Clusters in PCA Space (First Two Components)')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.show()\n\n\n\nAfter running K-Means clustering with a ‘k’ of 2, I decided to plot our clusters against the first two principal components. When doing PCA on the last tab, I saved a file with the linear combinations for each of the first two principal components. Now, when applying those linear combinations to each data point and classifying them with their K-Means cluster number, we can see this result. The first principal component seems to do a pretty efficient job of clustering our data, as the clusters are almost entirely divided down the x-axis.\n\n\nCode\npca_df['Cluster'] = pca_df['Cluster'].astype(str)\n\nfig = px.scatter_3d(pca_df, x='PCA1', y='PCA2', z='PCA3', color='Cluster', symbol='Cluster', opacity=0.7, size_max=10, title='3D Scatter Plot of PC1, PC2, and PC3 with Cluster Labels', labels={'0': 'Principal Component 1 (PC1)', '1': 'Principal Component 2 (PC2)','2': 'Principal Component 3 (PC3)', 'color': 'Cluster'}, color_discrete_sequence=['blue', 'green'])\n\nfig.update_layout(scene=dict(annotations=[dict(x=0, y=0, z=0, text='Cluster', showarrow=False)]))\nfig.update_layout(scene=dict(xaxis=dict(title_font=dict(size=16)),\n                             yaxis=dict(title_font=dict(size=16)),\n                             zaxis=dict(title_font=dict(size=16))))\n\nfig.show()\n\n\n\n                                                \n\n\nHere, I decided to include the third principal component and run the same analysis. Obviously, the first principal component remains dominant in determining cluster, but it is still cool to visualize the data in this way.\n\n\nCode\noptimal_k = 2\n\nkmeans = KMeans(n_clusters=optimal_k, random_state=101)\nlabels = kmeans.fit_predict(bart_standardized)\n\nbart_with_clusters = bart_target.copy()\nbart_with_clusters['Cluster'] = labels\nbart_with_clusters['Cluster'] = 1 - bart_with_clusters['Cluster'] \n\nalignment_count = bart_with_clusters.groupby(['Cluster', 'Made_Tournament']).size().reset_index(name='Count')\n\nmisalignment_counts = alignment_count[alignment_count['Cluster'] == alignment_count['Made_Tournament']]\nalignment_counts = alignment_count[alignment_count['Cluster'] != alignment_count['Made_Tournament']]\n\nprint(\"Counts for alignment:\")\nprint(alignment_counts)\n\nprint(\"\\nCounts for misalignment:\")\nprint(misalignment_counts)\n\n\nCounts for alignment:\n   Cluster  Made_Tournament  Count\n1        0                1    618\n2        1                0   1812\n\nCounts for misalignment:\n   Cluster  Made_Tournament  Count\n0        0                0   1071\n3        1                1     22\n\n\nNext, I wanted to see how closely our clustering resembled our Made Tournament column. The results here are interesting. 618 out of the 640 tournament teams in our dataset ended up in the same cluster (cluster 0). However, that cluster also contained 1071 teams that didn’t make the tournament. This makes sense to me as there really is no hard cutoff for tournament caliber teams and ones that are on the outside looking in. Most years results are fluid, and the last 10 or so teams that make it in could easily be exchanged for the next 10 or so that barely miss the cut. Our clustering algorithm wasn’t given specific numbers of points to put in each cluster, and because of this, the size of each cluster looks relatively similar (1689 for cluster 0 and 1834 for cluster 1). Considering only ~18% of our dataset made the tournament, there is a misalignment in sheer size.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.bar(alignment_counts['Cluster'].astype(str) + '_' + alignment_counts['Made_Tournament'].astype(str), alignment_counts['Count'], color='green', label='Alignment')\n\nplt.bar(misalignment_counts['Cluster'].astype(str) + '_' + misalignment_counts['Made_Tournament'].astype(str), misalignment_counts['Count'], color='red', label='Misalignment')\n\nplt.xlabel('Cluster_Made_Tournament')\nplt.ylabel('Count')\nplt.title('Alignment and Misalignment Counts')\nplt.legend()\nplt.show()\n\n\n\nThis is just a further visualization of what was discussed above. This bar chart does a good job of conveying that there were very few tournament teams (22 out of 640, ~3.4%) that were classified as cluster 1."
  },
  {
    "objectID": "clustering/clustering.html#dbscan",
    "href": "clustering/clustering.html#dbscan",
    "title": "Clustering",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is another clustering algorithm that seeks to find patterns of similarities within the data. DBSCAN differs from K-Means as it doesn’t require us to specify the number of clusters we want in advance.\nDBSCAN categorizes points into one of three categories: core points, border points, and noise points. Core points sit at, as the name suggests, the core of each cluster. They have a number of neighbors within a specified radius. Border points are in proximity to these core points but do not share the same density of points around them as core points do. Noise points are isolated points that aren’t classified as being part of any particular cluster.\nDBSCAN, because of these noise points, does a great job at handling outliers effectively. Since points do not have to be forced into a particular cluster, the clusters that are formed are often more compact. DBSCAN has two parameters- epsilon and minimum samples. Epsilon determines how far apart two points can be to remain in the same cluster. Minimum samples determines how many points need to be in a cluster for it to be designated as its own cluster.\nTo start, I looped through a range of epsilon values and minimum samples to determine which parameters led to the optimal silhouette score.\n\n\nCode\nbest_scores = []\nepsilons = []\nbest_clusters = []\n\neps_range = [i / 10 for i in range(25, 45)] \nmin_samples_range = range(2, 10)\n\nfor i in eps_range:\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    for j in min_samples_range:\n        model = DBSCAN(eps=i, min_samples=j)\n        predictions = model.fit_predict(bart_standardized)\n        num_clusters = len(set(predictions)) - (1 if -1 in predictions else 0)\n        if num_clusters &gt; 1: \n            score = silhouette_score(bart_standardized, predictions)\n            if score &gt; max_score:\n                max_score = score\n                best_cluster = num_clusters\n                best_eps = i\n\n    best_scores.append(max_score)\n    best_clusters.append(best_cluster)\n    epsilons.append(best_eps)\n\ndb_df = pd.DataFrame({'Epsilons': epsilons, 'Best_Clusters': best_clusters, 'Silhouette_Score': best_scores})\ndb_df = db_df[(db_df['Best_Clusters'] != -1) & (db_df['Silhouette_Score'] != -1)]\n\nplt.figure()\nsns.lineplot(data=db_df, x=\"Best_Clusters\", y=\"Silhouette_Score\")\nplt.title('Silhouette Score vs. Best Clusters (DBSCAN)')\nplt.xlabel('Best Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\nFrom these results, it once again seems like our silhouette score is maximized with 2 clusters. Looping through only the epsilon parameters of 2.5 to 4.5 seemed like a weird choice, but that range was found via trial and error. If the epsilon parameters are any lower, the silhouette scores quickly become negative, as our dataset has enough features and variance that most points become classified as outliers. For this reason, we need to increase the maximum distance allowed for two points, resulting in this analysis.\n\n\nCode\nprint(db_df.sort_values(by=\"Silhouette_Score\", ascending=False).head())\n\n\n    Epsilons  Best_Clusters  Silhouette_Score\n16       4.1              2          0.281549\n17       4.2              2          0.281549\n15       4.0              2          0.258917\n13       3.8              2          0.209236\n12       3.7              2          0.206638\n\n\nFrom this, we can see that all five of the maximum silhouette scores came from 2 clusters.\n\n\nCode\nmax_silhouette_score = db_df['Silhouette_Score'].max()\nmax_silhouette_row_index = db_df['Silhouette_Score'].idxmax()\nmax_silhouette_row = db_df.loc[max_silhouette_row_index]\n\nprint(\"Maximum Silhouette Score:\", max_silhouette_score)\n\n\nMaximum Silhouette Score: 0.28154948017583054\n\n\nHere, our maximum silhouette score via the DBSCAN method is 0.28."
  },
  {
    "objectID": "clustering/clustering.html#hierarchicalagglomerative-clustering",
    "href": "clustering/clustering.html#hierarchicalagglomerative-clustering",
    "title": "Clustering",
    "section": "",
    "text": "Hierarchical clustering is a clustering method that seeks to group data points into a hierarchical structure based on similarity. Hierarchical clustering does not require a predefined number of clusters, but instead produces a dendogram which shows a visual representation of the data’s relationship amongst itself.\nHierarchical clustering begins by treating each datapoint as its own, individual cluster. By iterating through the process, the algorithm begins to merge the most similar of these clusters, continuing to do so until there is one singular cluster with all of the datapoints. Inputs can vary based on different linkage and affinity criteria. To start, I will use Ward linkage, which seeks to minimize the variance between clusters. The affinity parameter determines how this distance is measured. I will be using Euclidean distance.\nHierarchical clustering is especially beneficial at revealing nested structures that may be present within the data. Because of this, it is easy to visualize different clusters at different scales. Depending on how many clusters one might want to use, it’s easy to see the divisions of these clusters when looking at the dendogram.\n\n\nCode\nnum_clusters = 2\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters= num_clusters, affinity='euclidean', linkage='ward') \nlabels = hierarchical_cluster.fit_predict(bart_standardized)\nprint(\"Cluster Labels total:\")\nprint(list(set(labels)))\n\n\nCluster Labels total:\n[0, 1]\n\n\nTo begin, I input the number of clusters as 2. This number of clusters will be used to label the leaves of the dendogram generated below.\n\n\nCode\nlinkage_matrix = linkage(bart_standardized, method='ward')\n\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, orientation='top', labels=labels, distance_sort='ascending', show_leaf_counts=True)\nplt.show()\n\n\n\nFrom the dendogram, we can see the hierarchical distinction between our two clusters. Cluster 1 looks to be considerably larger than cluster 0.\n\n\nCode\nsilhouette_scores = []\n\nfor n_clusters in range(2, 11):\n    hierarchical_cluster = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n    labels = hierarchical_cluster.fit_predict(bart_standardized)\n\n    if len(set(labels)) &gt; 1:\n        score = silhouette_score(bart_standardized, labels)\n        silhouette_scores.append({\"Number of Clusters\": n_clusters, \"Silhouette Score\": score})\n\nsilhouette_df = pd.DataFrame(silhouette_scores)\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=silhouette_df, x=\"Number of Clusters\", y=\"Silhouette Score\", marker=\"o\")\nplt.title('Silhouette Score vs. Number of Clusters (Agglomerative Clustering)')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\nWhile the first dendogram assumed two clusters was the ideal (as it was for K-Means and DBSCAN), I wanted to determine from the hierarchical silhouette score method if it determined 2 clusters was ideal for my dataset as well. After looping through the range of 2 to 10 clusters, it becomes pretty clear from the plot that the silhouette score is maximized at a cluster size of 2.\n\n\n\nIf this page taught us one thing, it would be that there seem to be two distinct clusters within our dataset. Each method disagrees to an extent about which specific points are in which cluster, but the size of two being optimal remains constant. While clustering is not a supervised learning method, I did want to see how closely the clusters aligned with which teams actually made the tournament. From the K-Means method, we were able to determine that our clustering does a good job at determining which teams have a shot to make the tournament. However, the boundary of where to separate the cluster is the big distinction. When splitting teams into 2 clusters, it seems clear that one of them is going to contain the majority of the more successful teams. However, that cluster seems to blend many more teams in with them too, implying that the boundary between tournament quality team and non-tournament quality team is blurry."
  },
  {
    "objectID": "data_cleaning/data_cleaning.html",
    "href": "data_cleaning/data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "In the Data Gathering tab, I imported a few different data sets, each of provides a different look at the concept of team stats within college basketball. Each data set contained far more columns than I ultimately will need, requiring me to do some basic data cleaning in order to make my data sets as each to use and manage as possible.\n\n\nAfter the initial data exploration, I saved the Creighton data set that was pulled from the “ncaahoopR” package on the Data Gathering tab. As previously mentioned, this data set has lots of unnecessary information, especially within the context of what I’m interested in.\n\n\nCode\ncreighton &lt;- read.csv(\"../data/creighton_2022_23.csv\")\nhead(creighton[, 1:5])\n\n\n    game_id       date                 home      away play_id\n1 401485099 2022-11-07 St. Thomas-Minnesota Creighton       1\n2 401485099 2022-11-07 St. Thomas-Minnesota Creighton       2\n3 401485099 2022-11-07 St. Thomas-Minnesota Creighton       3\n4 401485099 2022-11-07 St. Thomas-Minnesota Creighton       4\n5 401485099 2022-11-07 St. Thomas-Minnesota Creighton       5\n6 401485099 2022-11-07 St. Thomas-Minnesota Creighton       6\n\n\nNext, I wanted to add a column that explains whether or not Creighton scored on a particular play\n\n\nCode\n\nimport pandas as pd\n\ncreighton = pd.read_csv(\"../data/creighton_2022_23.csv\")\n\ndef is_creighton_score(row):\n    if row['home'] == 'Creighton' and row['home_score'] &gt; row['home_score_prev']:\n        return True\n    elif row['away'] == 'Creighton' and row['away_score'] &gt; row['away_score_prev']:\n        return True\n    else:\n        return False\n\ncreighton['home_score_prev'] = creighton.groupby('home')['home_score'].shift(1)\ncreighton['away_score_prev'] = creighton.groupby('away')['away_score'].shift(1)\ncreighton['Creighton_Score'] = creighton.apply(is_creighton_score, axis=1)\ncreighton.drop(['home_score_prev', 'away_score_prev'], axis=1, inplace=True)\ncreighton.to_csv(\"../data/creighton_2022_23_score.csv\", index=False)\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\ncreighton &lt;- read.csv(\"../data/creighton_2022_23_score.csv\")\n  \ncreighton_cleaned &lt;- creighton %&gt;%\n  select(home, away, secs_remaining_absolute, description, score_diff, scoring_play, win_prob, Creighton_Score) %&gt;%\n  mutate(time_left_mins = round(secs_remaining_absolute / 60, 2)) %&gt;%\n  select(-secs_remaining_absolute)\n\nwrite.csv(creighton_cleaned, \"../data/creighton_cleaned.csv\", row.names = FALSE)\n\nhead(creighton_cleaned, 10)\n\n\n                   home      away\n1  St. Thomas-Minnesota Creighton\n2  St. Thomas-Minnesota Creighton\n3  St. Thomas-Minnesota Creighton\n4  St. Thomas-Minnesota Creighton\n5  St. Thomas-Minnesota Creighton\n6  St. Thomas-Minnesota Creighton\n7  St. Thomas-Minnesota Creighton\n8  St. Thomas-Minnesota Creighton\n9  St. Thomas-Minnesota Creighton\n10 St. Thomas-Minnesota Creighton\n                                                    description score_diff\n1                                          Foul on Will Engels.          0\n2                               Trey Alexander made Free Throw.          1\n3                             Trey Alexander missed Free Throw.          1\n4                                Will Engels Defensive Rebound.          1\n5            Will Engels made Jumper. Assisted by Brooks Allen.         -1\n6                                   Arthur Kaluma missed Layup.         -1\n7                           Parker Bjorklund Defensive Rebound.         -1\n8                   Parker Bjorklund missed Three Point Jumper.         -1\n9                          Baylor Scheierman Defensive Rebound.         -1\n10 Ryan Kalkbrenner made Jumper. Assisted by Baylor Scheierman.          1\n   scoring_play  win_prob Creighton_Score time_left_mins\n1         False 0.5000000           False          39.72\n2          True 0.5222360           False          39.72\n3         False 0.5222360           False          39.72\n4         False 0.5222466           False          39.70\n5          True 0.4774954            True          39.30\n6         False 0.4773323           False          39.05\n7         False 0.4772994           False          39.00\n8         False 0.4772336           False          38.90\n9         False 0.4771897           False          38.83\n10         True 0.5228655           False          38.75\n\n\nAs you can see now, I filtered the data set to only have 8 features. This makes analysis much more feasible when I inevitably look to filter this data set further. The data also contains both qualitative and quantitative variables. It contains game situation information (score difference, time remaining, win probability), as well as a textual description of each play.\n\n\n\nAs I mentioned in the above section, the Creighton data set I’ve been using contains a description column full of text data. I thought it might be interesting to classify each row based on whether or not the player mentioned in each description is a member of Creighton’s roster or not.\n\n\nCode\n\nimport pandas as pd\n\ncreighton = pd.read_csv(\"../data/creighton_2022_23.csv\")\n\n\n\n\nCode\ncreighton_roster = [\"Jasen Green\", \"Ben Shtolzberg\", \"Ryan Nembhard\", \"Shereef Mitchell\", \"Francisco Farabello\", \"Zander Yates\", \"Ryan Kalkbrenner\", \"Mason Miller\", \"Sami Osmani\", \"John Christofilis\", \"Evan Young\", \"Devin Davis\", \"Trey Alexander\", \"Arthur Kaluma\", \"Fredrick King\", \"Baylor Scheierman\"]\n\nsearch_pattern= '|'.join(creighton_roster)\n\ncreighton_action = creighton[creighton['description'].apply(lambda x: any(roster in x for roster in creighton_roster))]\n\ncolumns_to_select= [\"description\", \"scoring_play\"]\n\ncreighton_action = creighton_action.loc[:, columns_to_select]\n\ncreighton_action.head(10)\n\n\n                                          description  scoring_play\n1                     Trey Alexander made Free Throw.          True\n2                   Trey Alexander missed Free Throw.         False\n5                         Arthur Kaluma missed Layup.         False\n8                Baylor Scheierman Defensive Rebound.         False\n9   Ryan Kalkbrenner made Jumper. Assisted by Bayl...          True\n10                          Foul on Ryan Kalkbrenner.         False\n13  Ryan Kalkbrenner made Layup. Assisted by Baylo...          True\n15                   Arthur Kaluma Defensive Rebound.         False\n16          Trey Alexander missed Three Point Jumper.         False\n17                   Arthur Kaluma Offensive Rebound.         False\n\n\n\n\nCode\nprint(\"Number of rows with Creighton player descriptions:\", creighton_action.shape[0])\n\n\nNumber of rows with Creighton player descriptions: 5009\n\n\nThis creates a new data frame, only with the descriptions containing Creighton player names as well as whether or not it was ultimately a scoring play. However, I’d rather add a column back into our cleaned data frame that contains whether or not it pertains to a Creighton player. This is what I do below.\n\n\nCode\ncreighton_cleaned = pd.read_csv(\"../data/creighton_cleaned.csv\")\n\ncreighton_roster = [\"Jasen Green\", \"Ben Shtolzberg\", \"Ryan Nembhard\", \"Shereef Mitchell\", \"Francisco Farabello\", \"Zander Yates\", \"Ryan Kalkbrenner\", \"Mason Miller\", \"Sami Osmani\", \"John Christofilis\", \"Evan Young\", \"Devin Davis\", \"Trey Alexander\", \"Arthur Kaluma\", \"Fredrick King\", \"Baylor Scheierman\"]\n\ncreighton_cleaned[\"creighton_data\"] = creighton_cleaned[\"description\"].str.contains(search_pattern)\n\ncreighton_cleaned.iloc[:, 1:10].head(10)\n\n\n        away  ... creighton_data\n0  Creighton  ...          False\n1  Creighton  ...           True\n2  Creighton  ...           True\n3  Creighton  ...          False\n4  Creighton  ...          False\n5  Creighton  ...           True\n6  Creighton  ...          False\n7  Creighton  ...          False\n8  Creighton  ...           True\n9  Creighton  ...           True\n\n[10 rows x 8 columns]\n\n\nPrinting the new creighton_cleaned data frame, we can see that the “creighton_data” column now contains information about whether or not a player on Creighton’s 2022-23 roster was involved in each play.\n\n\nCode\ncreighton_cleaned['players_involved'] = creighton_cleaned['description'].apply(lambda x: [name for name in creighton_roster if name in x] if any(name in x for name in creighton_roster) else None)\n\ncreighton_cleaned.iloc[:, 1:10].head(10)\n\n\n        away  ...                       players_involved\n0  Creighton  ...                                   None\n1  Creighton  ...                       [Trey Alexander]\n2  Creighton  ...                       [Trey Alexander]\n3  Creighton  ...                                   None\n4  Creighton  ...                                   None\n5  Creighton  ...                        [Arthur Kaluma]\n6  Creighton  ...                                   None\n7  Creighton  ...                                   None\n8  Creighton  ...                    [Baylor Scheierman]\n9  Creighton  ...  [Ryan Kalkbrenner, Baylor Scheierman]\n\n[10 rows x 9 columns]\n\n\nHere we can see the “players_involved” column that is added. It filters the description to find which players on the Creighton roster were involved in each play. If none were involved, the column returns None.\nLastly, I wanted to get the count of each player’s involvement to see who was a participant in the most plays.\n\n\nCode\nexpanded_df = creighton_cleaned.explode(\"players_involved\")\nplayer_counts = expanded_df[\"players_involved\"].value_counts()\n\nprint(player_counts)\n\n\nplayers_involved\nBaylor Scheierman      1020\nArthur Kaluma           925\nRyan Nembhard           895\nTrey Alexander          873\nRyan Kalkbrenner        841\nFredrick King           307\nFrancisco Farabello     232\nMason Miller            173\nShereef Mitchell        158\nBen Shtolzberg           69\nZander Yates             35\nSami Osmani              20\nEvan Young                5\nName: count, dtype: int64\n\n\nFrom this, we can see that Baylor Scheierman was the most involved in plays in Creighton’s 2022-23 season.\nFinally, we will save this cleaned data set for further use.\n\n\nCode\n\ncreighton_cleaned.to_csv(\"../data/creighton_cleaned.csv\", index= False)\n\n\n\nInitially, I had used Vectorizer to create this wordcloud. Unfortunately, I spent over six hours trying to fix an error about no usable init.tcl that refused to go away when running this codeblock. For this reason, I’ll leave the results but you will have to take my word for it that it was coded at one point.\nAs we can see, the most common words from the description tab seem to be rebound, jumper, and missed. This might come in handy when looking deeper into which specific team actions correlate with overall success.\nThese terms all seem to track as it appears to be a mix of common basketball occurrences as well as player names which were isolated above."
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#data-cleaning",
    "href": "data_cleaning/data_cleaning.html#data-cleaning",
    "title": "Data Cleaning",
    "section": "",
    "text": "In the Data Gathering tab, I imported a few different data sets, each of provides a different look at the concept of team stats within college basketball. Each data set contained far more columns than I ultimately will need, requiring me to do some basic data cleaning in order to make my data sets as each to use and manage as possible.\n\n\nAfter the initial data exploration, I saved the Creighton data set that was pulled from the “ncaahoopR” package on the Data Gathering tab. As previously mentioned, this data set has lots of unnecessary information, especially within the context of what I’m interested in.\n\n\nCode\ncreighton &lt;- read.csv(\"../data/creighton_2022_23.csv\")\nhead(creighton[, 1:5])\n\n\n    game_id       date                 home      away play_id\n1 401485099 2022-11-07 St. Thomas-Minnesota Creighton       1\n2 401485099 2022-11-07 St. Thomas-Minnesota Creighton       2\n3 401485099 2022-11-07 St. Thomas-Minnesota Creighton       3\n4 401485099 2022-11-07 St. Thomas-Minnesota Creighton       4\n5 401485099 2022-11-07 St. Thomas-Minnesota Creighton       5\n6 401485099 2022-11-07 St. Thomas-Minnesota Creighton       6\n\n\nNext, I wanted to add a column that explains whether or not Creighton scored on a particular play\n\n\nCode\n\nimport pandas as pd\n\ncreighton = pd.read_csv(\"../data/creighton_2022_23.csv\")\n\ndef is_creighton_score(row):\n    if row['home'] == 'Creighton' and row['home_score'] &gt; row['home_score_prev']:\n        return True\n    elif row['away'] == 'Creighton' and row['away_score'] &gt; row['away_score_prev']:\n        return True\n    else:\n        return False\n\ncreighton['home_score_prev'] = creighton.groupby('home')['home_score'].shift(1)\ncreighton['away_score_prev'] = creighton.groupby('away')['away_score'].shift(1)\ncreighton['Creighton_Score'] = creighton.apply(is_creighton_score, axis=1)\ncreighton.drop(['home_score_prev', 'away_score_prev'], axis=1, inplace=True)\ncreighton.to_csv(\"../data/creighton_2022_23_score.csv\", index=False)\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\ncreighton &lt;- read.csv(\"../data/creighton_2022_23_score.csv\")\n  \ncreighton_cleaned &lt;- creighton %&gt;%\n  select(home, away, secs_remaining_absolute, description, score_diff, scoring_play, win_prob, Creighton_Score) %&gt;%\n  mutate(time_left_mins = round(secs_remaining_absolute / 60, 2)) %&gt;%\n  select(-secs_remaining_absolute)\n\nwrite.csv(creighton_cleaned, \"../data/creighton_cleaned.csv\", row.names = FALSE)\n\nhead(creighton_cleaned, 10)\n\n\n                   home      away\n1  St. Thomas-Minnesota Creighton\n2  St. Thomas-Minnesota Creighton\n3  St. Thomas-Minnesota Creighton\n4  St. Thomas-Minnesota Creighton\n5  St. Thomas-Minnesota Creighton\n6  St. Thomas-Minnesota Creighton\n7  St. Thomas-Minnesota Creighton\n8  St. Thomas-Minnesota Creighton\n9  St. Thomas-Minnesota Creighton\n10 St. Thomas-Minnesota Creighton\n                                                    description score_diff\n1                                          Foul on Will Engels.          0\n2                               Trey Alexander made Free Throw.          1\n3                             Trey Alexander missed Free Throw.          1\n4                                Will Engels Defensive Rebound.          1\n5            Will Engels made Jumper. Assisted by Brooks Allen.         -1\n6                                   Arthur Kaluma missed Layup.         -1\n7                           Parker Bjorklund Defensive Rebound.         -1\n8                   Parker Bjorklund missed Three Point Jumper.         -1\n9                          Baylor Scheierman Defensive Rebound.         -1\n10 Ryan Kalkbrenner made Jumper. Assisted by Baylor Scheierman.          1\n   scoring_play  win_prob Creighton_Score time_left_mins\n1         False 0.5000000           False          39.72\n2          True 0.5222360           False          39.72\n3         False 0.5222360           False          39.72\n4         False 0.5222466           False          39.70\n5          True 0.4774954            True          39.30\n6         False 0.4773323           False          39.05\n7         False 0.4772994           False          39.00\n8         False 0.4772336           False          38.90\n9         False 0.4771897           False          38.83\n10         True 0.5228655           False          38.75\n\n\nAs you can see now, I filtered the data set to only have 8 features. This makes analysis much more feasible when I inevitably look to filter this data set further. The data also contains both qualitative and quantitative variables. It contains game situation information (score difference, time remaining, win probability), as well as a textual description of each play.\n\n\n\nAs I mentioned in the above section, the Creighton data set I’ve been using contains a description column full of text data. I thought it might be interesting to classify each row based on whether or not the player mentioned in each description is a member of Creighton’s roster or not.\n\n\nCode\n\nimport pandas as pd\n\ncreighton = pd.read_csv(\"../data/creighton_2022_23.csv\")\n\n\n\n\nCode\ncreighton_roster = [\"Jasen Green\", \"Ben Shtolzberg\", \"Ryan Nembhard\", \"Shereef Mitchell\", \"Francisco Farabello\", \"Zander Yates\", \"Ryan Kalkbrenner\", \"Mason Miller\", \"Sami Osmani\", \"John Christofilis\", \"Evan Young\", \"Devin Davis\", \"Trey Alexander\", \"Arthur Kaluma\", \"Fredrick King\", \"Baylor Scheierman\"]\n\nsearch_pattern= '|'.join(creighton_roster)\n\ncreighton_action = creighton[creighton['description'].apply(lambda x: any(roster in x for roster in creighton_roster))]\n\ncolumns_to_select= [\"description\", \"scoring_play\"]\n\ncreighton_action = creighton_action.loc[:, columns_to_select]\n\ncreighton_action.head(10)\n\n\n                                          description  scoring_play\n1                     Trey Alexander made Free Throw.          True\n2                   Trey Alexander missed Free Throw.         False\n5                         Arthur Kaluma missed Layup.         False\n8                Baylor Scheierman Defensive Rebound.         False\n9   Ryan Kalkbrenner made Jumper. Assisted by Bayl...          True\n10                          Foul on Ryan Kalkbrenner.         False\n13  Ryan Kalkbrenner made Layup. Assisted by Baylo...          True\n15                   Arthur Kaluma Defensive Rebound.         False\n16          Trey Alexander missed Three Point Jumper.         False\n17                   Arthur Kaluma Offensive Rebound.         False\n\n\n\n\nCode\nprint(\"Number of rows with Creighton player descriptions:\", creighton_action.shape[0])\n\n\nNumber of rows with Creighton player descriptions: 5009\n\n\nThis creates a new data frame, only with the descriptions containing Creighton player names as well as whether or not it was ultimately a scoring play. However, I’d rather add a column back into our cleaned data frame that contains whether or not it pertains to a Creighton player. This is what I do below.\n\n\nCode\ncreighton_cleaned = pd.read_csv(\"../data/creighton_cleaned.csv\")\n\ncreighton_roster = [\"Jasen Green\", \"Ben Shtolzberg\", \"Ryan Nembhard\", \"Shereef Mitchell\", \"Francisco Farabello\", \"Zander Yates\", \"Ryan Kalkbrenner\", \"Mason Miller\", \"Sami Osmani\", \"John Christofilis\", \"Evan Young\", \"Devin Davis\", \"Trey Alexander\", \"Arthur Kaluma\", \"Fredrick King\", \"Baylor Scheierman\"]\n\ncreighton_cleaned[\"creighton_data\"] = creighton_cleaned[\"description\"].str.contains(search_pattern)\n\ncreighton_cleaned.iloc[:, 1:10].head(10)\n\n\n        away  ... creighton_data\n0  Creighton  ...          False\n1  Creighton  ...           True\n2  Creighton  ...           True\n3  Creighton  ...          False\n4  Creighton  ...          False\n5  Creighton  ...           True\n6  Creighton  ...          False\n7  Creighton  ...          False\n8  Creighton  ...           True\n9  Creighton  ...           True\n\n[10 rows x 8 columns]\n\n\nPrinting the new creighton_cleaned data frame, we can see that the “creighton_data” column now contains information about whether or not a player on Creighton’s 2022-23 roster was involved in each play.\n\n\nCode\ncreighton_cleaned['players_involved'] = creighton_cleaned['description'].apply(lambda x: [name for name in creighton_roster if name in x] if any(name in x for name in creighton_roster) else None)\n\ncreighton_cleaned.iloc[:, 1:10].head(10)\n\n\n        away  ...                       players_involved\n0  Creighton  ...                                   None\n1  Creighton  ...                       [Trey Alexander]\n2  Creighton  ...                       [Trey Alexander]\n3  Creighton  ...                                   None\n4  Creighton  ...                                   None\n5  Creighton  ...                        [Arthur Kaluma]\n6  Creighton  ...                                   None\n7  Creighton  ...                                   None\n8  Creighton  ...                    [Baylor Scheierman]\n9  Creighton  ...  [Ryan Kalkbrenner, Baylor Scheierman]\n\n[10 rows x 9 columns]\n\n\nHere we can see the “players_involved” column that is added. It filters the description to find which players on the Creighton roster were involved in each play. If none were involved, the column returns None.\nLastly, I wanted to get the count of each player’s involvement to see who was a participant in the most plays.\n\n\nCode\nexpanded_df = creighton_cleaned.explode(\"players_involved\")\nplayer_counts = expanded_df[\"players_involved\"].value_counts()\n\nprint(player_counts)\n\n\nplayers_involved\nBaylor Scheierman      1020\nArthur Kaluma           925\nRyan Nembhard           895\nTrey Alexander          873\nRyan Kalkbrenner        841\nFredrick King           307\nFrancisco Farabello     232\nMason Miller            173\nShereef Mitchell        158\nBen Shtolzberg           69\nZander Yates             35\nSami Osmani              20\nEvan Young                5\nName: count, dtype: int64\n\n\nFrom this, we can see that Baylor Scheierman was the most involved in plays in Creighton’s 2022-23 season.\nFinally, we will save this cleaned data set for further use.\n\n\nCode\n\ncreighton_cleaned.to_csv(\"../data/creighton_cleaned.csv\", index= False)\n\n\n\nInitially, I had used Vectorizer to create this wordcloud. Unfortunately, I spent over six hours trying to fix an error about no usable init.tcl that refused to go away when running this codeblock. For this reason, I’ll leave the results but you will have to take my word for it that it was coded at one point.\nAs we can see, the most common words from the description tab seem to be rebound, jumper, and missed. This might come in handy when looking deeper into which specific team actions correlate with overall success.\nThese terms all seem to track as it appears to be a mix of common basketball occurrences as well as player names which were isolated above."
  },
  {
    "objectID": "decision_trees/decision_trees.html",
    "href": "decision_trees/decision_trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision Trees\n\nIntroduction to Decision Trees\nDecision Trees are a machine learning concept that seek to intuitively represent decision making processes. They can be used for both classification and regression and seek to break down the processes into manageable, interpretable chunks.\nA decision tree starts with a root node. The algorithm will find the specific feature split from the dataset that provides the most information about the target variable. There are a few methods of calculating these nodes. The one that I will use is Gini Impurity, which calculates how often a randomly chosen element would be incorrectly classified. The goal is to minimize Gini Impurity at each node, allowing for easier classification. Another metric that could be used to split a decision tree is Information Gain. This measures the reduction in entropy that occurs by splitting the dataset at a particular point.\nAfter the initial root node is discovered, the splitting point is also determined. For example, since we once again be using our NCAA data to classify whether or not teams made the tournament, our root node might look something like “W &gt; 20”. In this case, all teams with more than 20 wins would be split towards once branch, while the teams with 20 or less wins would be sent to a different branch. This process repeats itself for a certain number of levels, determined by the modeler. Having too many levels of a decision tree could lead to overfitting and needing to account for every edge case. Having too few levels of a decision tree might not accurately capture all of the important information contained in the dataset. The last level of the tree contains leaf nodes, which are used for prediction. On this page, I will be using a Decision Tree for classification, and thus these leaf nodes will seek to predict whether or not a team made the tournament based on above criteria.\n\nImports and Dataset\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn import metrics, tree \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nbart = pd.read_csv(\"../data/bart_dimensionality.csv\")\n\n\nOnce again, we have all of our usual imports. We will also be using lots of functions from the sklearn package to help us generate our decision trees as well as analyze them.\n\n\nCode\nclass_labels = {1: 'Missed', 0: 'Made'}\nplt.figure(figsize=(10, 6))\nax = sns.countplot(y='Made_Tournament', data=bart, palette='viridis')\nplt.title('Distribution of whether or not teams made the NCAA Tournament')\nplt.xlabel('Count')\nplt.ylabel('Made Tournament?')\nax.set_yticklabels([class_labels[label] for label in bart['Made_Tournament'].unique()])\nfor i, count in enumerate(bart['Made_Tournament'].value_counts()):\n    ax.text(count + 1, i, str(count), va='center', fontsize=10)\nplt.show()\n\n\n\nAfter a bit of data cleaning, I sought to visualize our target for classification. Once again, we will be seeking to classify teams based on whether or not they made the tournament. There is an imbalance between the sheer size of each of these two categories, meaning that our classifier should not seek to create an even split of both labels.\n\n\n\nRandom Classifier Function\n\n\nCode\ndef random_classifier(y_data, seed=None):\n    np.random.seed(seed)\n\n    ypred = np.random.randint(2, size=len(y_data))\n    \n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"Prediction Count:\", Counter(ypred).values())\n    print(\"Probability of Predictions:\", np.fromiter(Counter(ypred).values(), dtype=float) / len(y_data))\n    \n    accuracy = accuracy_score(y_data, ypred)\n    precision, recall, fscore, _ = precision_recall_fscore_support(y_data, ypred)\n    \n    print(\"Accuracy:\", accuracy)\n    print(\"Precision (Class 0, Class 1):\", precision)\n    print(\"Recall (Class 0, Class 1):\", recall)\n    print(\"F1-score (Class 0, Class 1):\", fscore)\n\ny = bart['Made_Tournament']\nrandom_classifier(y, seed=101)\n\n\n-----RANDOM CLASSIFIER-----\nPrediction Count: dict_values([1796, 1727])\nProbability of Predictions: [0.50979279 0.49020721]\nAccuracy: 0.5089412432585865\nPrecision (Class 0, Class 1): [0.83381587 0.19654788]\nRecall (Class 0, Class 1): [0.49947971 0.5515625 ]\nF1-score (Class 0, Class 1): [0.62472885 0.28981938]\n\n\nA random classifier model is created above. This random classifier will serve as our baseline. Its goal is to make predictions through random guessing. The classifier essentially generates a random number 1 or 0 for each of our points. The accuracy of this model was 50%, which means that it did its job. Since there is an imbalance between making and missing the tournament (~82% of teams miss), we will need a model that can also do something more meaningful than guess 0 and be right 82% of the time.\n\nDecision Tree\n\n\nCode\nfeature_columns = [col for col in bart.columns if col != 'Made_Tournament']\nX = bart[feature_columns].copy()\ntarget_column = ['Made_Tournament']\nY = bart[target_column].copy()\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1) \nmodel = DecisionTreeClassifier()\nmodel = model.fit(X_train,Y_train)\nY_pred = model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n\n\nAccuracy: 0.8845789971617786\n\n\nHere, we separate our data into training and test sets. The training data encompasses 70% of our dataset, while the test data takes up the remaining 30%. Here, we run the Decision Tree Classifier function on our training data to build our initial model. From this, we can see that our accuracy has shot up to over 88%. This is a significant improvement, meaning that our model classified 88% of teams correctly.\n\n\nCode\ndef custom_plot_tree(model, X, Y):\n    plt.figure(figsize=(22.5, 15))\n    plot_tree(model, feature_names=X.columns, filled=True)\n    plt.show()\n\ncustom_plot_tree(model, X_train, Y_train)\n\n\n\n\n\nHowever, our decision tree seems to be relatively large. At some points within the tree, it can get almost 20 levels deep, a sign of potential overfitting. To get around this, we will adjust our hyperparameters.\n\n\nCode\ntest_results = []\ntrain_results = []\n\nfor num_layer in range(1, 20):\n    model = DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(X_train, Y_train)\n\n    yp_train = model.predict(X_train)\n    yp_test = model.predict(X_test)\n\n    test_results.append([num_layer, accuracy_score(Y_test, yp_test), recall_score(Y_test, yp_test, pos_label=0),\n                         recall_score(Y_test, yp_test, pos_label=1)])\n    train_results.append([num_layer, accuracy_score(Y_train, yp_train), recall_score(Y_train, yp_train, pos_label=0),\n                          recall_score(Y_train, yp_train, pos_label=1)])\n\nnum_layers = [result[0] for result in test_results]\ntrain_accuracy_values = [result[1] for result in train_results]\ntest_accuracy_values = [result[1] for result in test_results]\ntrain_recall_0_values = [result[2] for result in train_results]\ntest_recall_0_values = [result[2] for result in test_results]\ntrain_recall_1_values = [result[3] for result in train_results]\ntest_recall_1_values = [result[3] for result in test_results]\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_accuracy_values, label='Training Accuracy', marker='o', color='green')\nplt.plot(num_layers, test_accuracy_values, label='Testing Accuracy', marker='o', color='purple')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Accuracy')\nplt.title('Training and Testing Accuracy vs. Number of Layers')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_0_values, label='Train Recall (y=0)', marker='o', color='green')\nplt.plot(num_layers, test_recall_0_values, label='Test Recall (y=0)', marker='o', color='purple')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall for y=0')\nplt.title('Training and Testing Recall for y=0 vs. Number of Layers')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_1_values, label='Train Recall (y=1)', marker='o', color='green')\nplt.plot(num_layers, test_recall_1_values, label='Test Recall (y=1)', marker='o', color='purple')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall for y=1')\nplt.title('Training and Testing Recall for y=1 vs. Number of Layers')\nplt.legend()\nplt.show()\n\n\n\n\n\nThe code above loops through a range of 1 to 20 layers and computes accuracy and recall of each. Our goal is to not overfit our model and to convey the greatest amount of critical information in the least amount of layers. To discern our optimal layer amount, we need to find the point where train accuracy and precision are relatively equal to the test accuracy and precision. After analyzing the three charts, this seems to be at a depth of only 2. With only 2 decisions being made, our tree might be overly simple, but lets test it out.\n\n\nCode\nmodel = tree.DecisionTreeClassifier(max_depth=2)\nmodel = model.fit(X_train, Y_train)\n\nyp_train = model.predict(X_train)\nyp_test = model.predict(X_test)\n\ncm_test = confusion_matrix(Y_test, yp_test)\nplt.figure(figsize=(12, 4))\n\nsns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16}, cbar=False,\n            xticklabels=['Missed Tournament', 'Made Tournament'], yticklabels=['Missed Tournament', 'Made Tournament'])\nplt.title('Confusion Matrix - Test Set')\nplt.xlabel('True Label') \nplt.ylabel('Predicted Label') \nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\n\nplt.show()\n\n\n\nAbove, we created a tree with a max depth of only 2 layers. Then, we plotted the confusion matrix. After just two splits, we can see that our model does a pretty solid job of classification. Lets run the specific metrics on just how well it did below.\n\n\nCode\naccuracy_test = accuracy_score(Y_test,yp_test)\nprecision_test = precision_score(Y_test, yp_test)\nrecall_test = recall_score(Y_test, yp_test)\n\nprint(\"\\nMetrics - Test Set:\")\nprint(f\"Accuracy: {accuracy_test:.4f}\")\nprint(f\"Precision: {precision_test:.4f}\")\nprint(f\"Recall: {recall_test:.4f}\")\n\n\n\nMetrics - Test Set:\nAccuracy: 0.9319\nPrecision: 0.8639\nRecall: 0.7095\n\n\nEven though there are only two splits in our decision tree, we achieve 93% accuracy. 985 of the 1057 teams in our test set were classified correctly. 127 of the 179 teams our model thought would make the tournament (86.4%) did, and 127 of the 147 teams who actually made the tournament were classified correctly.\n\n\nCode\nplt.figure(figsize=(6, 8))\nplot_tree(model, filled=True, feature_names=X_train.columns, class_names=['0', '1'], rounded=True)\nplt.title('Decision Tree Visualization')\nplt.savefig('dt1.png')\nplt.show()\n\n\n\nFrom the code above, we can now visualize the actual splits in our data. What is interesting to me is that our second split doesn’t actually matter at all. All of our classification is done after the first split. By classifying all teams with &gt;0.15 Wins Above Bubble as tournament teams and all others as non-tournament teams, the model is already able to achieve 93% accuracy.\n\n\n\nDecision Tree With No WAB, BARTHAG, or W\nTo challenge our decision tree, I decided to take out the WAB, BARTHAG, and Wins features from our data. I felt as though this makes the job of the decision tree too easy and doesn’t actually teach me much about which metrics are important in selection. We had already discovered the importance of these features on other tabs, so this time I wanted our model to struggle a little more.\n\n\nCode\nto_drop = [\"WAB\", \"W\", \"G\", \"BARTHAG\"]\nbart2 = bart.drop(columns= to_drop)\n\nfeature_columns = [col for col in bart2.columns if col != 'Made_Tournament']\nX = bart[feature_columns].copy()\n\ntarget_column = ['Made_Tournament']\nY = bart2[target_column].copy()\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1) \nmodel = DecisionTreeClassifier()\nmodel = model.fit(X_train,Y_train)\nY_pred = model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n\ntest_results = []\ntrain_results = []\n\nfor num_layer in range(1, 20):\n    model = DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(X_train, Y_train)\n\n    yp_train = model.predict(X_train)\n    yp_test = model.predict(X_test)\n\n    test_results.append([num_layer, accuracy_score(Y_test, yp_test), recall_score(Y_test, yp_test, pos_label=0),\n                         recall_score(Y_test, yp_test, pos_label=1)])\n    train_results.append([num_layer, accuracy_score(Y_train, yp_train), recall_score(Y_train, yp_train, pos_label=0),\n                          recall_score(Y_train, yp_train, pos_label=1)])\n\nnum_layers = [result[0] for result in test_results]\ntrain_accuracy_values = [result[1] for result in train_results]\ntest_accuracy_values = [result[1] for result in test_results]\ntrain_recall_0_values = [result[2] for result in train_results]\ntest_recall_0_values = [result[2] for result in test_results]\ntrain_recall_1_values = [result[3] for result in train_results]\ntest_recall_1_values = [result[3] for result in test_results]\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_accuracy_values, label='Training Accuracy', marker='o', color='green')\nplt.plot(num_layers, test_accuracy_values, label='Testing Accuracy', marker='o', color='purple')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Accuracy')\nplt.title('Training and Testing Accuracy vs. Number of Layers')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_0_values, label='Train Recall (y=0)', marker='o', color='green')\nplt.plot(num_layers, test_recall_0_values, label='Test Recall (y=0)', marker='o', color='purple')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall for y=0')\nplt.title('Training and Testing Recall for y=0 vs. Number of Layers')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_1_values, label='Train Recall (y=1)', marker='o', color='green')\nplt.plot(num_layers, test_recall_1_values, label='Test Recall (y=1)', marker='o', color='purple')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall for y=1')\nplt.title('Training and Testing Recall for y=1 vs. Number of Layers')\nplt.legend()\nplt.show()\n\n\n\n\n\nAfter excluding those columns and rerunning our model, we now see that the optimal layers for our decision tree should be 4. Further, our metrics never quite reach the same level as they did when we left WAB, W, and BARTHAG in. To me, these results are a lot more meaningful as they reflect specific metrics that a team can focus on, rather than metrics that are already an aggregation of others.\n\n\nCode\nmodel = tree.DecisionTreeClassifier(max_depth=4)\nmodel = model.fit(X_train, Y_train)\n\nyp_train = model.predict(X_train)\nyp_test = model.predict(X_test)\n\ncm_test = confusion_matrix(Y_test, yp_test)\nplt.figure(figsize=(12, 4))\n\nsns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16}, cbar=False,\n            xticklabels=['Missed Tournament', 'Made Tournament'], yticklabels=['Missed Tournament', 'Made Tournament'])\nplt.title('Confusion Matrix - Test Set')\nplt.xlabel('True Label') \nplt.ylabel('Predicted Label') \nplt.tight_layout()\nplt.savefig('confusion_matrix_3.png')\n\nplt.show()\n\n\n\n\n\nCode\naccuracy_test = accuracy_score(Y_test, yp_test)\nprecision_test = precision_score(Y_test, yp_test)\nrecall_test = recall_score(Y_test, yp_test)\n\nprint(\"\\nMetrics - Test Set:\")\nprint(f\"Accuracy: {accuracy_test:.4f}\")\nprint(f\"Precision: {precision_test:.4f}\")\nprint(f\"Recall: {recall_test:.4f}\")\n\n\n\nMetrics - Test Set:\nAccuracy: 0.9167\nPrecision: 0.8095\nRecall: 0.6648\n\n\nEven after four splits, our metrics are still lower than before. Our model was able to accurately predict the outcome for 969 of the 1057 teams (91.7%). This honestly is still rather impressive. 119 of the 179 teams our model thought would make the tournament (66.5%) did, and 119 of the 147 teams who actually made the tournament were classified correctly. Obviously this is a big step down from before, but since there were no cheating metrics to go off of, the model was forced to examine looser fitting patterns.\n\n\nCode\nplt.figure(figsize=(6, 8))\nplot_tree(model, filled=True, feature_names=X_train.columns, class_names=['0', '1'], rounded=True)\nplt.title('Decision Tree Visualization')\nplt.savefig('dt2.png')\nplt.show()\n\n\n\n\n\nCode\ntree_summary = tree.export_text(model, feature_names=X_train.columns.tolist())\nprint(tree_summary)\n\n\n|--- YearOffenseRank &lt;= 39.50\n|   |--- YearDefenseRank &lt;= 140.50\n|   |   |--- YearDefenseRank &lt;= 60.50\n|   |   |   |--- TOR &lt;= 21.80\n|   |   |   |   |--- class: 1\n|   |   |   |--- TOR &gt;  21.80\n|   |   |   |   |--- class: 0\n|   |   |--- YearDefenseRank &gt;  60.50\n|   |   |   |--- YearOffenseRank &lt;= 15.50\n|   |   |   |   |--- class: 1\n|   |   |   |--- YearOffenseRank &gt;  15.50\n|   |   |   |   |--- class: 1\n|   |--- YearDefenseRank &gt;  140.50\n|   |   |--- YearOffenseRank &lt;= 18.50\n|   |   |   |--- YearDefenseRank &lt;= 221.50\n|   |   |   |   |--- class: 1\n|   |   |   |--- YearDefenseRank &gt;  221.50\n|   |   |   |   |--- class: 0\n|   |   |--- YearOffenseRank &gt;  18.50\n|   |   |   |--- TORD &lt;= 18.05\n|   |   |   |   |--- class: 0\n|   |   |   |--- TORD &gt;  18.05\n|   |   |   |   |--- class: 0\n|--- YearOffenseRank &gt;  39.50\n|   |--- YearDefenseRank &lt;= 61.50\n|   |   |--- YearOffenseRank &lt;= 127.50\n|   |   |   |--- YearDefenseRank &lt;= 37.50\n|   |   |   |   |--- class: 1\n|   |   |   |--- YearDefenseRank &gt;  37.50\n|   |   |   |   |--- class: 0\n|   |   |--- YearOffenseRank &gt;  127.50\n|   |   |   |--- EFG_O &lt;= 51.50\n|   |   |   |   |--- class: 0\n|   |   |   |--- EFG_O &gt;  51.50\n|   |   |   |   |--- class: 1\n|   |--- YearDefenseRank &gt;  61.50\n|   |   |--- YearOffenseRank &lt;= 90.50\n|   |   |   |--- 2P_O &lt;= 52.45\n|   |   |   |   |--- class: 0\n|   |   |   |--- 2P_O &gt;  52.45\n|   |   |   |   |--- class: 0\n|   |   |--- YearOffenseRank &gt;  90.50\n|   |   |   |--- EFG_D &lt;= 49.45\n|   |   |   |   |--- class: 0\n|   |   |   |--- EFG_D &gt;  49.45\n|   |   |   |   |--- class: 0\n\n\n\nFrom this, we can see the new divisions our tree created. Our root node was YearOffenseRank. While being strong offensively was a good indicator of whether a team made the tournament, a team could still be incredibly lacking at defense and be not particularly good. An example of one of these teams would be 2023 Toledo, who was 5th in the country in offensive efficiency, yet was 294th in defense and missed the tournament entirely. 2023 Ohio State was 17th in the country in offense, but finished the season with a 16-19 record and were nowhere close to qualifying. Other metrics that appear are YearDefenseRank, Turnover Rate, Offensive and Defensive Effective Field Goal Percentage, and 2 Point Offense. Again, yearly offense and defense rankings seem to do most of the heavy lifting, so it might be worthwhile to remove those as well, but this model at least tells us much more about our dataset.\n\n\nConclusion\nUsing decision trees for classification is honestly some of the most fun I’ve had with this project. Because they’re so intuitive, it makes understanding underlying patterns in the data much easier. The big takeaways here are that the BARTHAG and WAB metrics essentially do a lot of the heavy lifting for my project, and someone much more experienced than me has done a far better job than I am currently able to. Regardless, removing those columns and checking out other important factors gave me a lot of context for what a team should look to prioritize. For my first time using decision trees, I walked away with a much better understanding of both how they work as well as their usefulness for classification."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis\nNow that our data is clean and well organized, we can begin to explore certain trends within it. My end goal for this project is to better understand the factors that aid teams in selection for the NCAA Men’s Basketball Tournament.\n\nWhat is Exploratory Data Analysis (EDA)?\nEDA is an important part of the data science life cycle. In order to begin to run any sort of modeling techniques, it’s necessary to have a deeper understanding of the actual data being presented. In this tab, I’ll dig deeper into the BartTorvik dataset which was introduced on the Data Gathering page. By looking at different aspects of the dataset (like sample statistics and variable correlation), it will make it easier to interpret the results found later in the project. All visualizations on this page should ideally be intuitive to any reader, regardless of whether or not they are familiar with college basketball.\n\nImports and Dataset\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfull_barttorvik = pd.read_csv(\"../data/cbb.csv\")\n\n\nThis EDA page will involve a lot of visualization, for which the plotly, matplotlib, and seaborn packages are incredibly helpful. I will also continue to use pandas to analyze and reshape my data.\n\n\n\nSummary Statistics\nAbove, we imported the packages necessary to manipulate and visualize the data on this tab. First, I thought it would be a good idea to view summary statistics about particular indicators I was interested in. I decided to select the 5 columns listed below:\n\nAdjusted Offensive Efficiency (ADJOE)- this is a metric that seeks to quantify how efficient a specific team is at scoring the basketball. The specific calculations that go into this metric can be found here. Essentially, it takes a team’s points per possession and adjusts them according to the quality of their opponent and venue. The average Division 1 team’s ADJOE usually comes out near 100, although this average differs by year.\nAdjusted Defensive Efficiency (ADJDE)- this metric is the inverse of above, essentially quantifying how good a team is at preventing their opponents from scoring. Because it is a defensive metric, a lower ADJDE is preferable, as opposed to a high ADJOE being preferable. Once again, the average team usually hovers around an ADJDE of 100, although it also differs by year.\nThree Point Percentage (3P_O)- this metric simply calculates a team’s three point percentage for the year. It tracks the number of three point shots made divided by total three point attempts.\nWins Above Bubble (WAB)- wins above bubble is a metric that seeks to quantify how good a team has performed from their schedule relative to a “bubble” team. In College Basketball, a bubble team typically refers to one of the Last Four In (last four teams to make the tournament) or First Four Out (first four teams left out of the tournament). There are 32 at large qualifier spots per year, meaning that the bubble usually encompasses the 29th-36th best at-large teams. This metric seeks to calculate, all things equal, how many more or fewer wins a team has attained compared to how the computer predicts an average bubble team would have performed with the same schedule. Because only 68 teams make the tournament each year (out of 350+), the mean WAB value will be significantly less than 0. Only the top ~50 or so teams for each given year will have a positive WAB.\nMade Tournament- this column was added during the data gathering portion of this project. Each team that ended up making the tournament (whether through auto qualifying due to winning their Conference Tournament or being selected as one of the 32 best at-large teams) received a 1 in this column, while all teams who missed the tournament received a 0.\n\n\n\nCode\nbarthag = full_barttorvik[\"BARTHAG\"]\ncolumns = [\"ADJOE\", \"ADJDE\", \"3P_O\", \"WAB\", \"Made_Tournament\"]\nfull_barttorvik[columns].describe()\n\n\n\n\n\n\n\n\n\nADJOE\nADJDE\n3P_O\nWAB\nMade_Tournament\n\n\n\n\ncount\n3523.000000\n3523.000000\n3523.000000\n3523.000000\n3523.000000\n\n\nmean\n103.151320\n103.153250\n34.185580\n-7.579620\n0.181663\n\n\nstd\n7.264859\n6.511989\n2.729186\n6.815976\n0.385622\n\n\nmin\n76.600000\n84.000000\n24.900000\n-25.200000\n0.000000\n\n\n25%\n98.200000\n98.400000\n32.300000\n-12.600000\n0.000000\n\n\n50%\n102.800000\n103.200000\n34.100000\n-7.900000\n0.000000\n\n\n75%\n107.900000\n107.800000\n36.000000\n-3.000000\n0.000000\n\n\nmax\n129.100000\n124.000000\n44.100000\n13.100000\n1.000000\n\n\n\n\n\n\n\nAfter analyzing the summary statistics, we can start to learn a few things about the dataset. First, we have data on 3523 distinct team seasons over the last decade. The average ADJOE and ADJDE were both a bit higher than 100, but are relatively equal (which is important for consistency’s sake). The average team shot 34.1% from three point range, while the best shooting team over the last ten years shot a staggering 44.1% from three. Our assumptions for WAB track, with the 75th percentile team still being 3 wins below the bubble.\nFinally, 18.2% of our 3523 teams made the tournament (640). This is interesting and allowed me to make a new realization about the data. For the last decade, the tournament has had four games played before the official start of the tournament to reduce the field from 68 –&gt; 64. Since this dataset only classifies 64 of teams per year as officially making the tournament, teams that lose their play-in game are considered as having missed the tournament for analysis purposes.\n\n\nVisualization\n\nEfficiency Metrics For The Last 10 Champions\n\n\nCode\nfilt1 = full_barttorvik.loc[full_barttorvik['POSTSEASON'] == 7].copy()\n\n\nfilt1['Label'] = filt1['TEAM'] + ' ' + filt1['YEAR'].astype(str)\n\n\nfig = px.scatter(\n    filt1,\n    x='ADJDE',\n    y='ADJOE',\n    text='Label', \n    title='ADJDE vs ADJOE for Last 10 Champions',\n    labels={'ADJDE': 'Adjusted Defensive Efficiency', 'ADJOE': 'Adjusted Offensive Efficiency'},\n)\n\nfig.update_traces(\n    textposition='top right',\n    mode='text+markers',\n    marker=dict(size=13),\n    textfont=dict(size=15)  \n)\n\nfig.update_layout(\n    margin=dict(l=0, r=0, t=40, b=0),\n    height=500,\n    xaxis=dict(\n        tickmode='linear', \n        dtick=5,\n        title=dict(text='Adjusted Defensive Efficiency', font=dict(size=14))\n    ),\n    yaxis=dict(\n        title=dict(text='Adjusted Offensive Efficiency', font=dict(size=14))\n    ),\n    title=dict(text='ADJDE vs ADJOE for Last 10 Champions', font=dict(size=24))\n)\n\n\nfig.show()\n\n\n\nIn this plot, I decided to compare the offensive and defensive efficiencies of the ten NCAA Champion teams from the dataset. As mentioned before, a high ADJOE and a low ADJDE is the ideal combination. As such, we should expect a team that dominates both aspects to be in the top left quadrant. However, the top left quadrant is pretty bare.\nInstead, we can see that Louisville’s 2013 team had by far the best defensive metrics of any team on this list. In contrast, Villanova’s 2018 team and Baylor’s 2021 team were more offensive minded, having the two lowest defensive efficiencies and some of the highest offensive efficiencies.\nWhile this is a fun plot, we can’t deduce a ton of meaningful information. The playstyle of basketball has changed significantly over the last decade, meaning that it’s not possible to perfectly compare these metrics year over year. To combat this, I created YearOffensiveRank and YearDefensiveRank columns. These simply compute how a team ranked relative to others their same year (1 being the best). Since my project analysis revolves around which teams make the tournament each year and is not concerned with which teams were the best across years, strictly comparing a team’s ADJOE and ADJDE to their yearly peers is sufficient.\n\n\nThree Point Percentage By Year\n\n\nCode\naverage_3p_o_by_year = full_barttorvik.groupby('YEAR')['3P_O'].mean().reset_index()\naverage_3p_o_by_year['3P_O'] = pd.to_numeric(average_3p_o_by_year['3P_O'], errors='coerce')\nplt.figure(figsize=(10, 6))\nsns.barplot(x='YEAR', y='3P_O', data=average_3p_o_by_year, hue = '3P_O', legend= False)\nplt.title('Average Team Three Point Percentage By Year')\nplt.xlabel('Year')\nplt.ylabel('Three Point Percentage')\nplt.xticks(rotation=45)\n\nfor index, value in enumerate(average_3p_o_by_year['3P_O']):\n    plt.text(index, value + 0.1, f'{value:.2f}', ha='center', va='bottom')\n\nplt.show()\n\n\n\nNext, I wanted to examine how much the average team’s three point percentage has changed by year. Initially, I expected there to be a lot more variation. Basketball has become much more three point reliant over the last decade. However, that doesn’t seem to be reflected in our data. The average team’s three point percentage hovered between 33 and 35% each year. While teams have become more reliant on the three, they aren’t necessarily shooting it at a higher clip. This chart also presents an opportunity to talk about 2020’s omission from the dataset. Due to COVID, the 2020 NCAA Basketball Tournament was cancelled, meaning that we will not be using that year’s data as there was no selection show or tournament.\n\n\nBARTHAG Distribution\n\n\nCode\ncustom_colors = sns.color_palette(\"viridis\")\n\nplt.figure()\nsns.histplot(full_barttorvik['BARTHAG'], bins=20, color=custom_colors[0], edgecolor='black', kde=False)\nplt.title('Distribution of BARTHAG Rating')\nplt.xlabel('Probability of Beating an Average D1 Opponent')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nNext, I wanted to take a look at the distribution of our dataset’s BARTHAG column. BARTHAG is the in-house power ranking computed by BartTorvik. Unfortunately (but understandably), their model is not public knowledge. However, we know it relies heavily on ADJOE and ADJDE. This indicator seeks to quantify how likely a team is to beat an average Division 1 opponent (essentially a team ranked ~180th each year) on a neutral court. As we can see from this distribution, it isn’t particularly normal. There seems to be a lot of rather poor performing teams that have a BARTHAG around ~0.3. The other values seem to be semi-uniformly distributed form 0.15-0.9.\n\n\nConferences With The Most Bids\n\n\nCode\ntop_conferences = full_barttorvik[full_barttorvik['Made_Tournament'] == 1]['CONF'].value_counts().nlargest(10).index\n\ndf_top_10_conferences = full_barttorvik[full_barttorvik['CONF'].isin(top_conferences)]\n\npercentage_data = df_top_10_conferences.groupby('CONF')['Made_Tournament'].mean() * 100\n\npercentage_data = percentage_data.sort_values(ascending=False)\n\ncustom_colors = sns.color_palette(\"flare\", len(percentage_data))\n\nplt.figure(figsize=(12, 5))\nax = sns.barplot(x=percentage_data.index, y=percentage_data.values, hue=custom_colors, legend=False)\n\nfor p in ax.patches:\n    percentage_value = p.get_height()\n    ax.annotate(f'{percentage_value:.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=10)\n\nplt.title('% of Tournament Teams from the 10 Conferences with the Most Bids')\nplt.xlabel('Conference')\nplt.ylabel('Percentage')\nplt.ylim(0, 100)\n\nplt.show()\n\n\n\nThe NCAA does not distribute bids equally across its 32 conferences. Of the 32 conferences, there are 6 classified as “Major Conferences” (Big 12, Big Ten, Big East, ACC, SEC, and Pac 12). These major conferences get a disproportionate amount of the at-large bids given out each year. This is due to a multitude of reasons, including that these conferences have, on average, a higher skill level due to much more funding. I wanted to explore how common it has been to make the tournament from each conference over the last decade. I only plotted the ten conferences with the most bids, partly due to ease of visualization but also due to the fact that nearly every conference outside of this top ten only receives their one automatic qualifier bid per year.\nFrom this, we can see that the Big 12 gets the highest proportion of their teams into the tournament. Over the last decade, 65% of individual seasons in the conference have resulted in a tournament appearance. Beyond the six major conferences, there are a few “High-Mid Majors” that typically receive a few bids per year. The American, Atlantic 10, Mountain West, and West Coast Conference usually send ~1/5 of their teams to dance each season.\nIts also worth noting that each conference has a differing number of teams. The Big 12, despite the name, has spent most of the last decade with just 10 members. The ACC is on the larger end, with 15 members. With impending realignment over the next few years, the number of schools in each of these major conferences will drastically change. This chart will certainly look much different a decade from now.\n\n\n\nCorrelation Analysis\nNext, I wanted to see how each of the columns in our dataset correlated with one another. Specifically, I wanted to see which metrics had the highest correlation with both making the tournament (Made_Tournament) and winning tournament games (POSTSEASON). To do so, I had to drop a lot of the columns that I didn’t feel made as much sense or were as interesting in terms of potential correlation. Our initial dataset contained 25 columns, so a correlation matrix that large would be hard to interpret.\n\n\nCode\nnumbered_bart = full_barttorvik.copy()\nto_drop = ['TEAM', 'ADJOE', 'ADJDE', '2P_O', '2P_D', 'YEAR', 'TotalRank', 'TotalOffenseRank', 'TotalDefenseRank', 'G', 'SEED', 'YearRank', 'POSTSEASON', 'ADJ_T', 'ORB', 'DRB', 'CONF']\nnumbered_bart.drop(columns= to_drop, inplace= True)\ncorrelation_matrix = numbered_bart.corr()\n\nplt.figure(figsize=(16, 14))\n\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, xticklabels=True, yticklabels=True, cbar_kws={'orientation': 'vertical'})\n\nplt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n\nplt.title('Correlation Matrix for full_barttorvik')\nplt.savefig('correlation_matrix.png', bbox_inches='tight')\n\nplt.show()\n\n\n\nFrom this matrix, we can immediately see a few strong relationships from our data. The BARTHAG metric seems to be heavily correlated with WAB, YearOffenseRank, and YearDefenseRank. None of this is surprising. One of the most surprising insights to me was the low correlation between making the tournament and defensive turnover rate. Teams better at forcing turnovers actually seem to have no change in their chance to make the tournament. Instead, the stronger correlation comes from offensive turnover rate (teams who take care of the ball and commit less turnovers are more likely to make the tournament).\nNext, I wanted to add the Postseason success column into the correlation matrix. This column, as created in the data gathering section, gives a value 0 through 7 to each team. A team with a value of 0 missed the tournament, while a team with a value of 7 won the entire thing. I decided to filter out all teams who missed the tournament for this matrix, hopefully to give a better glimpse into which factors remain important even after earning a spot in the tournament.\n\n\nCode\nnumbered_bart2 = full_barttorvik.copy()\nto_drop = ['TEAM', 'ADJOE', 'ADJDE', '2P_O', '2P_D', 'YEAR', 'TotalRank', 'TotalOffenseRank', 'TotalDefenseRank', 'G', 'YearRank', 'Made_Tournament',\n'ADJ_T', 'ORB', 'DRB', 'CONF']\nnumbered_bart2.drop(columns= to_drop, inplace= True)\nnumbered_bart2 = numbered_bart2[numbered_bart2['POSTSEASON'] != 0]\ncorrelation_matrix = numbered_bart2.corr()\n\nplt.figure(figsize=(16, 14))\n\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, xticklabels=True, yticklabels=True, cbar_kws={'orientation': 'vertical'})\n\nplt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n\nplt.title('Correlation Matrix for full_barttorvik')\nplt.savefig('correlation_matrix2.png', bbox_inches='tight')\n\nplt.show()\n\n\n\nSince this second correlation matrix filters out teams that didn’t make the tournament, it’s interesting to see what factors might be the differentiators for good teams. Unfortunately, for the sake of my personal curiosity, not much seems to change. Most of the correlations simply become slightly weaker, which makes sense considering that there is likely less variance in most of these values for teams good enough to make the tournament.\n\n\nHypothesis Generation/Moving Forward\nAfter all of this exploration, I am given a few potential leads for how to further investigate my initial hypothesis. From the beginning, I wanted to look into the factors that determine which teams do and don’t make the NCAA Tournament. After looking at visualizations and correlations, it seems as though BartTorvik provides some comprehensive predictive metrics, which do a good job of answering my initial question. A team’s offensive and defensive efficiencies lead to success. The BARTTHAG ranking does a good job of aggregating these efficiencies to create a predictive power ranking metric. However, I think digging deeper into specific other factors that influence these results would be really interesting. The dataset used provides over two dozen columns of statistical measures, many of which are components of a team’s efficiency metrics. Figuring out which of these building blocks carries the most influence will be something I’m excited to explore moving forward.\n\n\nData Refining/Outlier Analysis\nObviously, in a dataset of 3500+ samples, there are bound to be edge cases and outliers that don’t perfectly align with the rest of the data. Especially when considering that NCAA Tournament Selection is conducted by a committee, there is plenty of space for controversy and human error. Each year, there always seems to be a handful of surprise teams that end up getting at-large spots, as well as a few teams that seemingly had good enough metrics to have earned a spot in the tournament. While it’s undoubtedly heartbreaking for teams left on the outside looking in, it does make this project more engaging. There is no one objective ranking that determines which teams get in to the tournament. Instead, I hope to find a way to dig deeper into the human aspect of the selection process. Because of this, I actually think it is beneficial to leave all teams in my dataset and not remove outliers.\nWhile there might be a team that had a high enough number of Wins Above Bubble or a powerful enough offense to make the tournament in most instances, ultimately, there was a reason for each team not to have been selected. For this reason, I am not inclined to remove any teams from this dataset.\n\nSupposed Outliers\n\n\nCode\nplot_columns = [\"ADJOE\", \"ADJDE\"]\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=full_barttorvik[plot_columns], palette='Set2')\nplt.title('ADJOE and ADJDE Outlier Analysis')\nplt.ylabel('Values')\nplt.show()\n\n\n\nI decided to plot each team’s ADJOE and ADJDE to get a feel for any extreme outliers. Of course, had data been misinput, the outlier would be removed. However, we can see that each value in the dataset seems reasonable. Interestingly enough, there seem to be more teams that have been &gt;3 standard deviations from the mean for ADJOE than ADJDE. This may be something that gets looked into more down the road in this project.\n\n\n\nConclusion\nThrough Exploratory Data Analysis, I was able to get a better grasp on the dataset that I’ll continue to use throughout this project. Summary statistics, visuals, and correlation analyses were important in helping me decide where to take this project next."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In this project, I look to investigate questions relating to the NCAA Men’s Basketball Tournament selection process. Commonly known as March Madness, the tournament is hosted every March and April, consisting of 68 teams participating in a single-elimination format.\nOf those 68 teams, 32 qualify automatically by winning their individual conference tournament. The remaining 36 spots are “at-large” bids, given to teams deemed most worthy by a small selection committee. Many attempts have been made to model the committee’s decision making process, and I hope to investigate the specific attributes that the committee looks for when making their selections.\nAs of the 2023-24 season, there are 361 teams competing at the Division 1 level across 32 conferences. Each team plays an ~30-35 game season, consisting of ~10-15 non-conference opponents and the rest played against teams within their conference."
  },
  {
    "objectID": "introduction.html#project-overview",
    "href": "introduction.html#project-overview",
    "title": "Introduction",
    "section": "",
    "text": "In this project, I look to investigate questions relating to the NCAA Men’s Basketball Tournament selection process. Commonly known as March Madness, the tournament is hosted every March and April, consisting of 68 teams participating in a single-elimination format.\nOf those 68 teams, 32 qualify automatically by winning their individual conference tournament. The remaining 36 spots are “at-large” bids, given to teams deemed most worthy by a small selection committee. Many attempts have been made to model the committee’s decision making process, and I hope to investigate the specific attributes that the committee looks for when making their selections.\nAs of the 2023-24 season, there are 361 teams competing at the Division 1 level across 32 conferences. Each team plays an ~30-35 game season, consisting of ~10-15 non-conference opponents and the rest played against teams within their conference."
  },
  {
    "objectID": "introduction.html#questions-to-answer",
    "href": "introduction.html#questions-to-answer",
    "title": "Introduction",
    "section": "Questions To Answer",
    "text": "Questions To Answer\n\nWhich factors are most important for NCAA Tournament selection?\nDoes playing an easier non-conference schedule punish teams competing for at-large bids?\nWho are the biggest tournament snubs of all time?\nWho are the worst tournament selections of all time?\nDoes historical tournament success lead to more favorable future seeding?\nWhat role do efficiency metrics play in tournament seeding?\nAre end of season trends weighed more heavily during tournament selection?\nHow many teams per year get auto-bids who wouldn’t have otherwise qualified for the tournament?\nWhat role does human bias play in tournament seeding?\nWould automating tournament selection be advisable?\nWhat role does seeding play in actual tournament success?\nDo efficiency metrics lead to a self-fulfilling feedback loop once the season reaches conference play?"
  },
  {
    "objectID": "introduction.html#previous-research",
    "href": "introduction.html#previous-research",
    "title": "Introduction",
    "section": "Previous Research",
    "text": "Previous Research\nA few research papers have been written about the NCAA Tournament Selection Process. Here are the summaries of two of them.\n\nEvidence of bias in NCAA tournament selection and seeding (Coleman, DuMond, and Lynch 2010)\nThis paper uses 41 different data sets collected from the 1999-2008 seasons to see how well predictive metrics correlated with actual tournament seeing. They perform this analysis to investigate if certain conferences/teams have unfair bias impacting their seeding outcome. They ultimately find that there is substantial evidence of bias, especially in favor of teams who have some sort of representation within the Selection Committee.\nIt is interesting to note that the tournament format and metrics looked at have changed over the time period since this paper was written. Three more at large teams now get bids to the tournament, and the leading efficiency metrics used in College Basketball had not been adopted for use by the committee at this point in time. It will be interesting to see how things have changed since this paper was written.\n\n\nModeling the NCAA basketball tournament selection process using a decision tree (Dutta and Jacobson 2018)\nThe paper introduces a decision tree model aiming to replicate the NCAA basketball tournament’s team selection process. The author’s model attempts to mimic this process by utilizing expert rankings and performance metrics, including RPI , top 50 wins, rank 200+ losses, strength of schedule, games against top 100 teams, and recent performance. The model was evaluated using data from 2012 to 2016 and ultimately demonstrated a high level of accuracy in selecting teams for the tournament.\nThe paper mentions that models will never be able to fully predict the human element of the tournament selection. Individual bias is hard to account for, but this retroactive fitting model is an interesting way to look at things. As formats and metrics continue to change, I wonder how successful a past-looking model is at being able to predict the future but there is a lot of valuable information here. Ultimately, I found this approach to be pretty interesting and will likely hope to model a similar approach during my project."
  }
]